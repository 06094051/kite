
<!DOCTYPE html>
<!--
 Generated by Apache Maven Doxia at 2014-03-18
 Rendered using Maven Reflow Skin 1.0.0 (http://andriusvelykis.github.com/reflow-maven-skin)
-->
<html  xml:lang="en" lang="en">

	<head>
		<meta charset="UTF-8" />
		<title>Kite Data Module - Kite Data Reference Guide</title>
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<meta name="description" content="" />
		<meta http-equiv="content-language" content="en" />

		<link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/css/bootstrap.min.css" rel="stylesheet" />
		<link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
		<link href="./css/docs.css" rel="stylesheet" />
		<link href="./css/reflow-skin.css" rel="stylesheet" />
		
		<link href="http://yandex.st/highlightjs/7.3/styles/github.min.css" rel="stylesheet" />
		
		<link href="./css/lightbox.css" rel="stylesheet" />
		
		<link href="./css/site.css" rel="stylesheet" />
		<link href="./css/print.css" rel="stylesheet" media="print" />
		
		<!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
		<!--[if lt IE 9]>
			<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

	</head>

	<body class="page-guide project-kite-data" data-spy="scroll" data-offset="60" data-target="#toc-scroll-target">

		<div class="navbar navbar-fixed-top">
			<div class="navbar-inner">
				<div class="container">
					<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</a>
					<div class="brand">Kite Software Development Kit</div>
					<div class="nav-collapse">
						<ul class="nav pull-right">
							<li class="dropdown">
								<a href="#" class="dropdown-toggle" data-toggle="dropdown">Main <b class="caret"></b></a>
								<ul class="dropdown-menu">
									<li><a href="../index.html" title="Home">Home </a></li>
									<li><a href="../release_notes.html" title="Release Notes">Release Notes </a></li>
									<li><a href="../apidocs/index.html" title="Javadoc">Javadoc </a></li>
									<li><a href="../jdiff/changes.html" title="API Diffs">API Diffs </a></li>
									<li><a href="../dependencies.html" title="Dependency Information">Dependency Information </a></li>
									<li><a href="../migrating.html" title="Migrating from CDK">Migrating from CDK </a></li>
									<li><a href="http://github.com/kite-sdk/kite-examples" title="Examples" class="externalLink">Examples </a></li>
								</ul>
							</li>
							<li class="dropdown">
								<a href="#" class="dropdown-toggle" data-toggle="dropdown">Modules <b class="caret"></b></a>
								<ul class="dropdown-menu">
									<li><a href="index.html" title="Kite Data">Kite Data </a></li>
									<li><a href="../kite-maven-plugin/index.html" title="Kite Maven Plugin">Kite Maven Plugin </a></li>
									<li><a href="../kite-morphlines/index.html" title="Kite Morphlines">Kite Morphlines </a></li>
									<li><a href="../kite-tools/index.html" title="Kite Tools">Kite Tools </a></li>
								</ul>
							</li>
							<li class="dropdown active">
								<a href="#" class="dropdown-toggle" data-toggle="dropdown">Kite Data <b class="caret"></b></a>
								<ul class="dropdown-menu">
									<li><a href="index.html" title="Home">Home </a></li>
									<li class="active"><a href="" title="Reference Guide">Reference Guide </a></li>
									<li><a href="design.html" title="Design">Design </a></li>
									<li><a href="limitations.html" title="Known Limitations">Known Limitations </a></li>
									<li><a href="faq.html" title="FAQ">FAQ </a></li>
								</ul>
							</li>
							<li class="dropdown">
								<a href="#" class="dropdown-toggle" data-toggle="dropdown">Contribute <b class="caret"></b></a>
								<ul class="dropdown-menu">
									<li><a href="http://github.com/kite-sdk/kite" title="Source Repository" class="externalLink">Source Repository </a></li>
									<li><a href="https://issues.cloudera.org/browse/CDK" title="JIRA" class="externalLink">JIRA </a></li>
									<li><a href="https://groups.google.com/a/cloudera.org/forum/#!forum/cdk-dev" title="Mailing List" class="externalLink">Mailing List </a></li>
									<li><a href="../how_to_contribute.html" title="How To Contribute">How To Contribute </a></li>
									<li><a href="../adding_a_module.html" title="Adding a Module">Adding a Module </a></li>
									<li><a href="../project-info.html" title="Project Information">Project Information </a></li>
								</ul>
							</li>
						</ul>
					</div><!--/.nav-collapse -->
				</div>
			</div>
		</div>
		
	<div class="container">
	
	<!-- Masthead
	================================================== -->
	</header>

	<div class="main-body">
	<div class="row">
		<div class="span12">
			<div class="body-content">
<div class="page-header">
 <h1>Kite Data Reference Guide</h1>
</div> 
<div class="section"> 
 <h2 id="About_This_Guide">About This Guide</h2> 
 <p>This reference guide is the primary source of documentation for the Kite Data module. It covers the following topics: * high level organization of the APIs * primary classes and interfaces * intended usage * available extension points for customization * implementation information (where helpful and appropriate)</p> 
 <p>From here on, this guide assumes you are already familiar with the basic design and functionality of the following technologies: * HDFS * Hadoop MapReduce * Java SE 6 </p> 
 <p>If you are familiar with <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a>, data serialization techniques, common compression algorithms (for example, Gzip, Snappy), advanced Hadoop MapReduce topics (for example, input split calculation), and traditional data management topics (for example, partitioning schemes, metadata management), you will benefit even more.</p> 
</div> 
<div class="section"> 
 <h2 id="Overview_of_the_Data_Module">Overview of the Data Module</h2> 
 <p>The Kite Data module is a set of APIs for interacting with datasets in the Hadoop ecosystem. It is specifically built to handle direct reading and writing of datasets in storage subsystems such as the Hadoop Distributed FileSystem (HDFS). The Kite Data module provides familiar, stream-oriented and random-access APIs that reduce the complexity of data serialization, partitioning, organization, and metadata system integration.</p> 
 <p>These APIs do not replace or supersede any of the existing Hadoop APIs. Instead, the Data module acts as a targeted application of those APIs for its stated use case. You will still use the HDFS or Avro APIs directly when you have use cases outside of direct dataset create, delete, read, and write operations. When you are building applications or systems such as data integration services, the Kite Data module is usually superior in its default choices, data organization, and metadata system integration, when compared to custom-built code.</p> 
 <p>In keeping with the overarching theme and principles of Kite, the Data module is prescriptive. Rather than present a do-all Swiss Army knife library, this module makes specific design choices that guide you toward well known patterns that make sense for many, if not all, cases. If you have advanced or niche use cases, you might find it difficult, suboptimal, or even impossible to do unusual things. </p> 
 <p>Limiting your options is not the goal. The Kite Data module is designed to be immediately useful, obvious, and in line with what most users want out of the box. Whenever revealing an option creates complexity, or otherwise requires you to research and assess additional choices, the option is omitted.</p> 
 <p>These APIs are designed to easily fit in with dependency injection frameworks such as <a class="externalLink" href="http://www.springsource.org/spring-framework" title="Spring Framework">Spring</a> and <a class="externalLink" href="http://code.google.com/p/google-guice/" title="Google Guice">Google Guice</a>. You can use constructor injection when using these kinds of systems. Alternatively, if you prefer not to use DI frameworks, you can use the builder-style helper classes that come with many of the critical classes. By convention, these builders are always inner static classes named <tt>Builder</tt>, contained within their constituent classes.</p> 
 <p>The primary actors in the Data module are <i>entities</i>, <i>dataset repositories</i>, <i>datasets</i>, dataset <i>readers</i>, dataset <i>writers</i>, and <i>metadata providers</i>. Most of these objects are interfaces, permitting multiple implementations, each with different functionality. The current release contains an implementation of each of these components for the Hadoop FileSystem abstraction (found in the <tt>org.kitesdk.data.filesystem</tt> package), for Hive (found in the <tt>org.kitesdk.data.hcatalog</tt> package), and for HBase (see the section about Dataset Repository URIs for how to access it).</p> 
 <p>While, in theory, any implementation of Hadoop’s <tt>FileSystem</tt> abstract class is supported by the Kite Data module, only the local and HDFS filesystem implementations are tested and officially supported.</p> 
 <p>If you’re not already familiar with <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a> schemas, now is a good time to go read a little <a class="externalLink" href="http://avro.apache.org/docs/current/spec.html" title="Avro Specification">more about them</a>. You don’t have to worry about the details of how objects are serialized, but you must be able to specify the schema to which entities of a dataset must conform. The rest of this guide assumes you know how to define a schema.</p> 
</div> 
<div class="section"> 
 <h2 id="Dataset_Repositories_and_Metadata_Providers">Dataset Repositories and Metadata Providers</h2> 
 <p>A <i>dataset repository</i> is a physical storage location for datasets. In keeping with the relational database analogy, a dataset repository is the equivalent of a database of tables. You can organize datasets into different dataset repositories for reasons related to logical grouping, security and access control, backup policies, and so on. A dataset repository is represented by instances of the <tt>org.kitesdk.data.DatasetRepository</tt> interface in the Kite Data module. An instance of <tt>DatasetRepository</tt> acts as a factory for datasets, supplying methods for creating, loading, and deleting datasets. Each dataset belongs to exactly one dataset repository. There’s no built-in support for moving or copying datasets between repositories. MapReduce and other execution engines provide copy functionality, if required.</p> 
 <p><i>DatasetRepository Interface</i></p> 
 <div class="source"> 
  <pre>&lt;E&gt; Dataset&lt;E&gt; create(String, DatasetDescriptor);
&lt;E&gt; Dataset&lt;E&gt; load(String);
&lt;E&gt; Dataset&lt;E&gt; update(String);
boolean delete(String);
boolean exists(String);
Collection&lt;String&gt; list();
</pre> 
 </div> 
 <p>The Kite Data module ships with a <tt>DatasetRepository</tt> implementation <tt>org.kitesdk.data.filesystem.FileSystemDatasetRepository</tt> built for operating on datasets stored in a filesystem supported by Hadoop’s <tt>FileSystem</tt> abstraction. This implementation requires a root directory under which datasets are stored, and a <tt>Configuration</tt> used to get the <tt>FileSystem</tt> for that directory. Optionally, you can supply a <i>metadata provider</i> as well. With a <tt>DatasetRepository</tt>, you can freely interact with datasets while the implementation manages files and directories in the underlying filesystem. The metadata provider is used to store and retrieve dataset schemas and other related information needed to read and write data (more about them later).</p> 
 <p>The Kite Data module supports URI-based instantiation to build <tt>DatasetRepository</tt> instances. The following code example opens a <tt>DatasetRepository</tt> that stores its data in HDFS.</p> 
 <div class="source"> 
  <pre>// get a repository with data stored in hdfs:/data
DatasetRepository hdfsRepo = DatasetRepositories.open(&quot;repo:hdfs:/data&quot;);
</pre> 
 </div> 
 <p>For more information, see [the repository URI section](#Dataset Repository URIs).</p> 
 <p>Instantiating <tt>FileSystemDatasetRepository</tt> using its builder is also straightforward, and supports additional options such as supplying a specific Hadoop <tt>Configuration</tt>. This demonstrates building a <tt>DatasetRepository</tt> in a <tt>Configured</tt> class, like <tt>Tool</tt>:</p> 
 <div class="source"> 
  <pre>DatasetRepository hdfsRepo = new FileSystemDatasetRepository.Builder()
  .configuration(this.getConf())
  .rootDirectory(new Path(&quot;/data&quot;))
  .build();
</pre> 
 </div> 
 <p>This example uses the currently configured default Hadoop <tt>FileSystem</tt>, typically an HDFS cluster. Since Hadoop also supports a “local” implementation of <tt>FileSystem</tt>, it’s possible to use the Data APIs to interact with data residing on a local OS filesystem. This is especially useful during development and basic functional testing of your code. The <tt>Path</tt> object tells the repository builder the path and configured filesystem for data storage.</p> 
 <div class="source"> 
  <pre>DatasetRepository localRepo = new FileSystemDatasetRepository.Builder()
  .configuration(getConf())
  .rootDirectory(new Path(&quot;file:/tmp/test-data&quot;))
  .build();

// alternative URI-based instantiation:
localRepo = DatasetRepositories.open(&quot;repo:file:/data&quot;)
</pre> 
 </div> 
 <p>Using these instances of <tt>DatasetRepository</tt>, you can create new datasets, and existing datasets can be loaded or deleted. Here’s a more complete example of creating a dataset to store application event data. You’ll notice a few new classes that are discussed later in this document.</p> 
 <div class="source"> 
  <pre>// Instantiate a DatasetRepository backed by HDFS, stored under /data
DatasetRepository repo = DatasetRepositories.open(&quot;repo:hdfs:/data&quot;)

// Create the dataset &quot;users&quot; with the schema defined in the file User.avsc.
Dataset users = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .build()
);
</pre> 
 </div> 
 <p>Related to the dataset repository, the <i>metadata provider</i> stores information needed to read from and write to datasets in a repository. That information is created with a dataset and after that is managed by the repository and its metadata provider.</p> 
 <p>The providers implement <tt>org.kitesdk.data.MetadataProvider</tt>, which defines a service provider interface used to interact with a service that provides dataset metadata information to the rest of the Data APIs. This interface defines the contract that metadata services must provide to the library, and specifically, the <tt>DatasetRepository</tt>.</p> 
 <p><i>MetadataProvider Interface</i></p> 
 <div class="source"> 
  <pre>DatasetDescriptor create(String, DatasetDescriptor);
DatasetDescriptor load(String);
DatasetDescriptor update(String, DatasetDescriptor);
boolean delete(String);
boolean exists(String);
</pre> 
 </div> 
 <p><tt>MetadataProvider</tt> implementations act as a bridge between the Data module and centralized metadata repositories. An obvious example of this (in the Hadoop ecosystem) is <a class="externalLink" href="http://incubator.apache.org/hcatalog/" title="Apache HCatalog">HCatalog</a> and the Hive metastore. By providing an implementation that makes the necessary API calls to HCatalog’s REST service, any and all datasets are immediately consumable by systems compatible with HCatalog, the storage system represented by the <tt>DatasetRepository</tt> implementation, and the format in which the data is written. As it turns out, that’s a pretty tall order. In keeping with Kite’s goal of reducing rather than adding options, you are encouraged to 1. use HCatalog 2. allow this library to default to Snappy-compressed Avro data files 3. use systems that also integrate with HCatalog (directly or indirectly).</p> 
 <p>In this way, this library acts as a fourth integration point for working with data in HDFS that is HCatalog-aware, in addition to Hive, Pig, and MapReduce input/output formats.</p> 
 <p>You aren’t expected to use metadata providers directly. Typically, the URI used to open a <tt>DatasetRepository</tt> also holds information about the metadata provider.</p> 
 <p>Most <tt>DatasetRepository</tt> implementations accept instances of <tt>MetadataProvider</tt> plugins, and make whatever calls are needed as users interact with the Data APIs. You might have noticed that no metadata provider is specified when the code instantiated <tt>FileSystemDatasetRepository</tt>. That’s because <tt>FileSystemDatasetRepository</tt> uses an implementation of <tt>MetadataProvider</tt> called <tt>FileSystemMetadataProvider</tt> by default. You are free to explicitly pass a different implementation using the <tt>metadataProvider(MetadataProvider)</tt> method on <tt>FileSystemDatasetRepository.Builder</tt> if you want to change this behavior.</p> 
 <p>The <tt>FileSystemMetadataProvider</tt> (also in the package <tt>org.kitesdk.data.filesystem</tt>) plugin stores dataset metadata information on a Hadoop <tt>FileSystem</tt> in a hidden directory. As with its sibling <tt>FileSystemDatasetRepository</tt>, its constructor accepts a Hadoop <tt>FileSystem</tt> object and a base directory. When you need to store metadata, <tt>FileSystemMetadataProvider</tt> creates a directory under the supplied base directory with the dataset name (if it doesn’t yet exist), and serializes the dataset descriptor information to a set of files in a directory named <tt>.metadata</tt>.</p> 
 <p><i>Example: Explicitly configuring <tt>FileSystemDatasetRepository</tt> with <tt>FileSystemMetadataProvider</tt></i></p> 
 <div class="source"> 
  <pre>FileSystem fileSystem = FileSystem.get(new Configuration());
Path basePath = new Path(&quot;/data&quot;);

MetadataProvider metaProvider = new FileSystemMetadataProvider(
  fileSystem, basePath);

DatasetRepository repo = new FileSystemDatasetRepository.Builder()
  .configuration(getConf())
  .metadataProvider(metaProvider)
  .build();
</pre> 
 </div> 
 <p>Configured this way, data and metadata are stored together, side by side, on whatever filesystem Hadoop is currently configured to use. Later, when you create a dataset, you’ll see the resultant file and directory structure created as a result of this configuration.</p> 
 <p>It’s very common to store metadata in the Hive/HCatalog Metastore (the terms are used interchangeably), since this opens datasets up to integration with any system that can work with Hive, such as BI tools, or Cloudera Impala.</p> 
 <p>When using Hive/HCatalog you have two options for specifying the location of the data files. You can let Hive/HCatalog manage the location of the data, the so-called “managed tables” option, in which case the data is stored in the warehouse directory that is configured by the Hive/HCatalog installation (see the <tt>hive.metastore.warehouse.dir</tt> setting in <i>hive-site.xml</i>). Alternatively, you can provide an explicit Hadoop <tt>FileSystem</tt> and root directory for datasets, just like <tt>FileSystemDatasetRepository</tt>. The latter option is referred to as “external tables” in the context of Hive/HCatalog.</p> 
 <p><i>Example: Creating a <tt>HCatalogDatasetRepository</tt> with managed tables</i></p> 
 <div class="source"> 
  <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hive&quot;);
</pre> 
 </div> 
 <p><i>Example: Creating a <tt>HCatalogDatasetRepository</tt> with external tables</i></p> 
 <div class="source"> 
  <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hive:/data&quot;);
</pre> 
 </div> 
 <div class="section"> 
  <h3 id="Dataset_Repository_URIs">Dataset Repository URIs</h3> 
  <p>Dataset repositories can be referenced by URI, using the <tt>repo</tt> URI scheme. The following table lists the supported URI formats. See the <tt>DatasetRepositories</tt> Javadoc for more information on the URI format.</p> 
  <table border="0" class="bodyTable table table-striped table-hover"> 
   <thead> 
    <tr class="a"> 
     <th>Dataset Repository Implementation </th> 
     <th>URI format </th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr class="b"> 
     <td>Local filesystem </td> 
     <td><tt>repo:file:[path]</tt> </td> 
    </tr> 
    <tr class="a"> 
     <td>HDFS </td> 
     <td><tt>repo:hdfs://[host]:[port]/[path]</tt> </td> 
    </tr> 
    <tr class="b"> 
     <td>Hive/HCatalog with managed tables </td> 
     <td><tt>repo:hive</tt> or <tt>repo:hive://[metastore-host]:[metastore-port]</tt> </td> 
    </tr> 
    <tr class="a"> 
     <td>Hive/HCatalog with external tables </td> 
     <td><tt>repo:hive://[ms-host]:[ms-port]/[path]?hdfs-host=[host]&amp;hdfs-port=[port]</tt> </td> 
    </tr> 
    <tr class="b"> 
     <td>HBase (random access) </td> 
     <td><tt>repo:hbase:[zookeeper-host1],[zookeeper-host2],[zookeeper-host3]</tt> </td> 
    </tr> 
   </tbody> 
  </table> 
  <p>The <tt>DatasetRepositories</tt> class in the <tt>org.kitesdk.data</tt> package provides factory methods for retrieving a <tt>DatasetRepository</tt> instance for a URI. For almost all cases, this is the preferred method of retrieving an instance of a <tt>DatasetRepository</tt>.</p> 
  <p><i>DatasetRepositories Interface</i></p> 
  <div class="source"> 
   <pre>static DatasetRepository open(URI);
static DatasetRepository open(String);

static RandomAccessDatasetRepository openRandomAccess(URI);
static RandomAccessDatasetRepository openRandomAccess(String);
</pre> 
  </div> 
  <p><i>Example: Creating a <tt>DatasetRepository</tt> for Hive managed tables from a dataset repository URI</i></p> 
  <div class="source"> 
   <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hive&quot;);
</pre> 
  </div> 
  <p><i>Example: Creating a <tt>DatasetRepository</tt> for HBase from a repository URI</i></p> 
  <div class="source"> 
   <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hbase:zk1,zk2,zk3&quot;);
</pre> 
  </div> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="Datasets">Datasets</h2> 
 <p><i>Summary</i></p> 
 <ul> 
  <li>A dataset is a collection of entities.</li> 
  <li>A dataset is represented by the interface <tt>Dataset</tt>.</li> 
  <li>The Hadoop FileSystem implementation of a dataset… 
   <ul> 
    <li>is stored as Snappy-compressed Avro data files by default, or optionally in the column-oriented Parquet file format</li> 
    <li>is made up of zero or more files in a directory.</li> 
   </ul></li> 
  <li>You can work with a subset of Dataset Entities using the <tt>Views</tt> API.</li> 
 </ul> 
 <p>A dataset is a collection of zero or more entities. All datasets have a name and an associated <i>dataset descriptor</i>. The dataset descriptor, as the name implies, describes all aspects of the dataset. Primarily, the descriptor information is the dataset’s required <i>schema</i> and <i>format</i>, and its optional <i>location</i> (a repository URI) and optional <i>partition strategy</i>. A descriptor must be provided at the time a dataset is created. The schema is defined using the Avro Schema APIs.</p> 
 <p>Entities must all conform to the same schema; however, that schema can evolve based on a set of well defined rules. The relational database analog of a dataset is a table.</p> 
 <p>Datasets are represented by the <tt>org.kitesdk.data.Dataset</tt> interface, which is parameterized by the Java type of the entities it is used to read and write.</p> 
 <p><i>Dataset Interface for entity type &lt;E&gt;</i></p> 
 <div class="source"> 
  <pre>String getName();
DatasetDescriptor getDescriptor();

DatasetWriter&lt;E&gt; newWriter();
DatasetReader&lt;E&gt; newReader();

Dataset getPartition(PartitionKey, boolean);
Iterable&lt;Dataset&lt;E&gt;&gt; getPartitions();
</pre> 
 </div> 
 <p>Up to this point, this example has omitted the <tt>Dataset</tt> type parameter for brevity. To demonstrate the correct use of types, the remaining examples include the type parameter whenever you create or load a <tt>Dataset</tt>.</p> 
 <div class="source"> 
  <pre>DatasetRepository repo = ...
Dataset&lt;User&gt; users = repo.load(&quot;users&quot;);
</pre> 
 </div> 
 <p><tt>Dataset</tt> implementations decide how to physically store the entities within the dataset. You do not instantiate implementations of the <tt>Dataset</tt> interface directly. Instead, implementations of the <tt>DatasetRepository</tt> act as a factory for the appropriate <tt>Dataset</tt> implementation.</p> 
 <p>The included Hadoop <tt>FileSystemDatasetRepository</tt> provides a <tt>Dataset</tt> implementation called <tt>FileSystemDataset</tt>. This dataset implementation stores data in the configured Hadoop <tt>FileSystem</tt> as Snappy-compressed Avro data files, or optionally as Parquet files.</p> 
 <p>Avro data files were selected as the default because they: * are supported by all components of CDH * are language agnostic * support block compression * have a compact binary representation * are natively splittable by Hadoop MapReduce while compressed.</p> 
 <p>Parquet, on the other hand, is a good choice for wide tables with a large number of columns (30 or so is considered “large” in this context), particularly if the data is queried using Impala, since Impala can take advantage of the fact that Parquet is stored in columnar form and restricts the data being read to the columns in the query.</p> 
 <p>Upon creation of dataset, you must provide a name and a <i>dataset descriptor</i> to the <tt>DatasetRepository#create()</tt> method. The descriptor, represented by the <tt>org.kitesdk.data.DatasetDescriptor</tt> class, holds all metadata associated with the dataset, the most important of which is the schema. Schemas are always represented using Avro’s Schema APIs, regardless of how the data is stored by the underlying dataset implementation. This simplifies the API, letting you focus on a single schema definition language for all datasets. In an effort to support different styles of schema definition, the <tt>DatasetDescriptor.Builder</tt> class supports a number of convenience methods for defining or attaching a schema.</p> 
 <p><i>DatasetDescriptor Class</i></p> 
 <div class="source"> 
  <pre>org.apache.avro.Schema getSchema();
Format getFormat()
URI getLocation()
PartitionStrategy getPartitionStrategy();
boolean isPartitioned();
</pre> 
 </div> 
 <p><i>DatasetDescriptor.Builder Class</i></p> 
 <div class="source"> 
  <pre>Builder schema(Schema schema);
Builder schema(File file);
Builder schema(InputStream inputStream);
Builder schemaUri(URI uri);
Builder schemaUri(String uri);
Builder schemaLiteral(String json);

Builder partitionStrategy(PartitionStrategy partitionStrategy);

DatasetDescriptor build();
</pre> 
 </div> 
 <p><i>Note</i></p> 
 <p>Some of the less important or more specialized methods have been elided here in the interest of simplicity.</p> 
 <p>From the methods in the <tt>DatasetDescriptor.Builder</tt>, you can see Avro schemas can be defined in a few different ways. Here, for instance, is an example of creating a dataset with a schema defined in a file on the local filesystem.</p> 
 <div class="source"> 
  <pre>DatasetRepository repo = ...
Dataset&lt;User&gt; users = repo.create(&quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .build()
);
</pre> 
 </div> 
 <p>Just as easily, a schema could be loaded from a Java classpath resource. This example uses Guava’s <a class="externalLink" href="http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Resources.html" title="Google Guava Resources class">Resources</a> for resolution, but you could almost as easily use Java’s <tt>java.util.ClassLoader</tt> directly.</p> 
 <div class="source"> 
  <pre>DatasetRepository repo = ...
Dataset users = repo.create(&quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(Resources.getResource(&quot;Users.avsc&quot;))
    .build()
);
</pre> 
 </div> 
 <p>An instance of <tt>Dataset</tt> acts as a factory for both reader and writer streams. Each implementation is free to produce stream implementations that make sense for the underlying storage system. The <tt>FileSystemDataset</tt> implementation, for example, produces streams that read from, or write to, Avro data files or Parquet files on a Hadoop <tt>FileSystem</tt> implementation.</p> 
 <p>Reader and writer streams both function similarly to Java’s standard IO streams, but are specialized. As indicated in the <tt>Dataset</tt> interface earlier, both interfaces are generic. The type parameter indicates the type of entity that they produce or consume, respectively.</p> 
 <p><i>DatasetReader Interface</i></p> 
 <div class="source"> 
  <pre>void open();
void close();
boolean isOpen();

boolean hasNext();
E next();
</pre> 
 </div> 
 <p><i>DatasetWriter Interface</i></p> 
 <div class="source"> 
  <pre>void open();
void close();
boolean isOpen();

void write(E);
void flush();
</pre> 
 </div> 
 <p>Both readers and writers are single-use objects with a well defined lifecycle. You must open instances of both types (or the implementations of each, rather) prior to invoking any of the IO-generating methods such as DatasetReader’s <tt>hasNext()</tt> or <tt>next()</tt>, or DatasetWriter’s <tt>write()</tt> or <tt>flush()</tt>. Once a stream is closed via the <tt>close()</tt> method, no further IO is permitted, and you cannot reopen the stream.</p> 
 <p>Writing to a dataset always follows the same sequence of events. You obtain an instance of a <tt>Dataset</tt> from a <tt>DatasetRepository</tt>, either by creating a new or loading an existing dataset. With a reference to a <tt>Dataset</tt>, you can obtain a writer using its <tt>newWriter()</tt> method, open it, write any number of entities, flush as necessary, and close it to release resources back to the system. The use of <tt>flush()</tt> and <tt>close()</tt> can dramatically affect data durability. Implementations of the <tt>DatasetWriter</tt> interface are free to define the semantics of data durability as appropriate for their storage subsystem. See the implementation Javadoc on either the streams or the dataset for more information.</p> 
 <p><i>Example: Writing to a Hadoop FileSystem</i></p> 
 <div class="source"> 
  <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hdfs:/data&quot;);

/*
 * Let's assume MyInteger.avsc is defined as follows:
 * {
 *   &quot;type&quot;: &quot;record&quot;,
 *   &quot;name&quot;: &quot;MyInteger&quot;,
 *   &quot;fields&quot;: [
 *     { &quot;type&quot;: &quot;integer&quot;, &quot;name&quot;: &quot;i&quot; }
 *   ]
 * }
 */
Dataset&lt;GenericRecord&gt; integers = repo.create(&quot;integers&quot;,
  new DatasetDescriptor.Builder()
    .schema(&quot;MyInteger.avsc&quot;)
    .build()
);

/*
 * Getting a writer never performs IO, so it's safe to do this outside of
 * the try block. Here we're using Avro Generic records, discussed in
 * greater details later. See the Entities section.
 */
DatasetWriter&lt;GenericRecord&gt; writer = integers.newWriter();

try {
  writer.open();

  for (int i = 0; i &lt; Integer.MAX_VALUE; i++) {
    writer.write(
      new GenericRecordBuilder(integers.getDescriptor().getSchema())
        .set(&quot;i&quot;, i)
        .build()
    );
  }
} finally {
  // Always explicitly close writers!
  writer.close();
}
</pre> 
 </div> 
 <p>Reading data from an existing dataset is equally straightforward. DatasetReaders implement the standard <a class="externalLink" href="http://docs.oracle.com/javase/7/docs/api/java/util/Iterator.html">Iterator</a> and <a class="externalLink" href="http://docs.oracle.com/javase/7/docs/api/java/util/Iterable.html">Iterable</a> interfaces, so Java’s for-each syntax works and is the easiest way to get entities from a reader. If calling <tt>next()</tt> without using for-each syntax, keep in mind that it is incorrect to call <tt>next()</tt> after the reader is exhausted (that is, no more entities remain) and an exception is thrown. Instead, you must use the <tt>hasNext()</tt> method to test if the reader can produce further data.</p> 
 <p><i>Example: Reading from a Hadoop FileSystem</i></p> 
 <div class="source"> 
  <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hdfs:/data&quot;);

// Load the integers dataset.
Dataset&lt;GenericReader&gt; integers = repo.get(&quot;integers&quot;);

DatasetReader&lt;GenericRecord&gt; reader = integers.newReader();

try {
  reader.open();

  for (GenericRecord record : reader) {
    System.out.println(&quot;i: &quot; + record.get(&quot;i&quot;));
  }
} finally {
  reader.close();
}
</pre> 
 </div> 
 <p>Deleting a dataset — an operation as destructive as dropping a table in a relational database — works as expected.</p> 
 <p><i>Example: Deleting an existing dataset</i></p> 
 <div class="source"> 
  <pre>DatasetRepository repo = DatasetRepositories.open(&quot;repo:hdfs:/data&quot;);

if (repo.delete(&quot;integers&quot;)) {
  System.out.println(&quot;Deleted dataset integers&quot;);
}
</pre> 
 </div> 
 <p>As discussed earlier, all operations performed on dataset repositories, datasets, and their associated readers and writers are tightly integrated with the dataset repository’s configured metadata provider. Deleting a dataset like this, for example, removes the data as well as the associated metadata. All applications that use the Data module APIs automatically see changes made by one another if they share the same configuration. This is an incredibly powerful concept, allowing systems to become immediately aware of data as soon as it’s committed to storage.</p> 
 <div class="section"> 
  <h3 id="Partitioned_Datasets">Partitioned Datasets</h3> 
  <p><i>Summary</i></p> 
  <ul> 
   <li>Datasets can be partitioned by attributes of the entity (that is, fields of the record).</li> 
   <li>Partitioning is transparent to readers and writers.</li> 
   <li>Partitions also conform to the <tt>Dataset</tt> interface.</li> 
   <li>A <tt>PartitionStrategy</tt> controls how a dataset is partitioned, and is part of the <tt>DatasetDescriptor</tt>.</li> 
  </ul> 
  <p>You can optionally partition Datasets to facilitate piecemeal storage management, as well as optimized access to data under one or more predicates. A dataset is considered partitioned if it has an associated partition strategy (described later).</p> 
  <p>When you write entities to a partitioned dataset, they are automatically written to the proper partition, as expected. The semantics of a partition are defined by the implementation; no guarantees as to the performance of reading or writing across partitions, availability of a partition in the face of failures, or the efficiency of partition elimination under one or more predicates (that is, partition pruning in query engines) are made by the Data module interfaces. It is not possible to partition an existing non-partitioned dataset, nor can you write data into a partitioned dataset that does not land in a partition. Should you decide to partition an existing dataset, the best course of action is to create a new partitioned dataset with the same schema as the existing dataset, and use MapReduce to convert the dataset in batch to the new format. A partitioned dataset can provide a list of partitions (described later).</p> 
  <p>When you create a dataset, you can provide a <tt>PartitionStrategy</tt>. A partition strategy is a list of one or more partition functions that, when applied to an attribute of an entity, produce a value used to decide in which partition an entity should be written. Different partition function implementations exist, each of which facilitates a different form of partitioning. The library includes identity, hash, and date functions for use in partition strategies.</p> 
  <p><tt>PartitionStrategy</tt> has a <tt>Builder</tt> interface to create partition strategy instances.</p> 
  <p><i>PartitionStrategy.Builder API</i></p> 
  <div class="source"> 
   <pre>&lt;S&gt; Builder identity(String, Class&lt;S&gt;, int);
Builder hash(String, int);
Builder year(String);
Builder month(String);
Builder day(String);
Builder hour(String);
Builder minute(String);
Builder dateFormat(String, String);

PartitionStrategy build();
</pre> 
  </div> 
  <p>When building a partition strategy, you specify the attribute (or field) name from which to take the function input, along with a cardinality hint (or limit, in the case of the hash function). For example, given the Avro schema for a <tt>User</tt> entity with a <tt>segment</tt> attribute of type <tt>long</tt>, a partition strategy that uses the identity function on the <tt>segment</tt> attribute effectively “buckets” users by their segment value.</p> 
  <p><i>Sample User Avro Schema (User.avsc)</i></p> 
  <div class="source"> 
   <pre>{
  &quot;name&quot;: &quot;User&quot;,
  &quot;type&quot;: &quot;record&quot;,
  &quot;fields&quot;: [
    { &quot;name&quot;: &quot;id&quot;,           &quot;type&quot;: &quot;long&quot;   },
    { &quot;name&quot;: &quot;username&quot;,     &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot;: &quot;emailAddress&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot;: &quot;segment&quot;,      &quot;type&quot;: &quot;long&quot;   }
  ]
}
</pre> 
  </div> 
  <p><i>Example Creation of a dataset partitioned by an attribute</i></p> 
  <div class="source"> 
   <pre>DatasetRepository repo = ...

Dataset&lt;User&gt; usersDataset = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .partitionStrategy(
      new PartitionStrategy.Builder().identity(&quot;segment&quot;, Long.class, 1024).build()
    ).build()
);
</pre> 
  </div> 
  <p>Given the fictitious <tt>User</tt> entities shown in <i>Example: Sample Users</i>, users A, B, and C are written to partition 1, while D and E are written to partition 2.</p> 
  <p><i>Example: Sample Users</i></p> 
  <div class="source"> 
   <pre>id  username  emailAddress  segment
--  --------  ------------  -------
100 A         A@a.com       1
101 B         B@b.com       1
102 C         C@c.com       1
103 D         D@d.com       2
104 E         E@e.com       2
</pre> 
  </div> 
  <p>Partitioning is not limited to a single attribute of an entity.</p> 
  <p><i>Example: Creation of a dataset partitioned by multiple attributes</i></p> 
  <div class="source"> 
   <pre>DatasetRepository repo = ...

Dataset&lt;User&gt; users = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .partitionStrategy(
      new PartitionStrategy.Builder()
        .identity(&quot;segment&quot;, Long.class, 1024)  // Partition first by segment
        .hash(&quot;emailAddress&quot;, 3)                // and then by hash(email) % 3
        .build()
    ).build()
</pre> 
  </div> 
  <p>The order in which you add partition functions is important. This controls the way the data is physically partitioned in certain implementations of the Data APIs. Depending on the implementation, this can drastically change the execution speed of data access by different methods.</p> 
  <p><b>Warning</b></p> 
  <p>It’s worth pointing out that Hive and Impala only support the identity function in partitioned datasets, at least at the time this is written. If you do not use partitioning for subset selection, you can use any partition function(s) you choose. If, however, you want to use the partition pruning in Hive/Impala’s query engine, only the identity function will work. This is because both systems rely on the idea that the value in the path name equals the value found in each record. To mimic more complex partitioning schemes, you might need to add a surrogate field to each record to hold the derived value and handle proper setting of such a field yourself.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Random_Access_Datasets">Random Access Datasets</h3> 
  <p>Datasets stored in HBase support random access read and write operations, as well as the usual streaming read and write operations. The random-access pattern associates a <tt>Key</tt> with each record that is stored (covered next), and the <tt>Dataset</tt> implementations implement an additional interface, <tt>RandomAccessDataset</tt>:</p> 
  <p><i>RandomAccessDataset Interface</i></p> 
  <div class="source"> 
   <pre>E get(Key);
boolean put(E);
boolean delete(Key);
boolean delete(E);
long increment(Key, String, long);
</pre> 
  </div> 
  <p>The type parameter, <i>E</i>, is the Java type of the entities stored in the <tt>Dataset</tt>.</p> 
  <p>Access these methods by first instantiating the HBase dataset repository as a <tt>RandomAccessDatasetRepository</tt>, which returns <tt>RandomAccessDataset</tt> implementations rather than vanilla <tt>Dataset</tt> implementations:</p> 
  <div class="source"> 
   <pre>RandomAccessDatasetRepository repo = DatasetRepositories.openRandomAccess(&quot;repo:hbase:...&quot;);
RandomAccessDataset&lt;User&gt; users = repo.open(&quot;users&quot;);
users.put(...);
</pre> 
  </div> 
  <p>The underlying HBase storage requires that each entity has a key associated with it; in the current release, you define the key by marking the key fields in the schema with the <tt>key</tt> mapping. You specify non-key fields by the <tt>column</tt> mapping, with a value indicating the HBase column family and column name.</p> 
  <p><i>Example: User entity schema with mappings</i></p> 
  <div class="source"> 
   <pre>Avro schema (User.avsc)
-----------------------
{
  &quot;name&quot;: &quot;User&quot;,
  &quot;type&quot;: &quot;record&quot;,
  &quot;fields&quot;: [
    { &quot;name&quot;: &quot;username&quot;,    &quot;type&quot;: &quot;string&quot;,
      &quot;mapping&quot;: { &quot;type&quot;: &quot;key&quot;, &quot;value&quot;: &quot;0&quot; } },

    { &quot;name&quot;: &quot;emailAddress&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;null&quot; ],
      &quot;mapping&quot;: { &quot;type&quot;: &quot;column&quot;, &quot;value&quot;: &quot;user:emailAddress&quot; } },
  ]
}
</pre> 
  </div> 
  <p>You add entities to a dataset using <tt>put</tt>, retrieve them by key with <tt>get</tt>, and remove them from the dataset with <tt>delete</tt>. The overloaded form of <tt>delete</tt> that takes an entity is a conditional delete that only performs the delete if the entity’s version field is the same as the one in the store (see Optimistic Concurrency Control, discussed below). The <tt>increment</tt> method performs an atomic increment on a named <tt>int</tt> or <tt>long</tt> field, which must be mapped as a “counter” (see mapping table below).</p> 
  <p>Keys are represented by the <tt>Key</tt> class, constructed via a <tt>Builder</tt>. All of the schema’s key fields are required to construct a <tt>Key</tt>, and the builder throws an <tt>IllegalStateException</tt> if a field is missing.</p> 
  <p><i>Schema Mapping</i></p> 
  <p>The “column” and “key” mappings used above demonstrate how you configure fields in the entity schema to be stored in HBase. A “mapping” is required for each field and indicates how to store that field. The table below shows the different mapping types and their definitions:</p> 
  <table border="0" class="bodyTable table table-striped table-hover"> 
   <thead> 
    <tr class="a"> 
     <th>Mapping type </th> 
     <th>Details </th> 
     <th>Example </th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr class="b"> 
     <td>key </td> 
     <td>Store the field in the row key; position is set by the value </td> 
     <td><tt>{&quot;type&quot;: &quot;key&quot;, &quot;value&quot;: &quot;0&quot;}</tt> </td> 
    </tr> 
    <tr class="a"> 
     <td>column </td> 
     <td>Store the field in the family:qualifier given by the value </td> 
     <td><tt>{&quot;type&quot;: &quot;column&quot;, &quot;value&quot;: &quot;fam:qual&quot;}</tt> </td> 
    </tr> 
    <tr class="b"> 
     <td>keyAsColumn </td> 
     <td>Store a map or record with the value as the column family and key/field names as qualifiers </td> 
     <td><tt>{&quot;type&quot;: &quot;keyAsColumn&quot;, &quot;value&quot;: &quot;fam:&quot;}</tt> </td> 
    </tr> 
    <tr class="a"> 
     <td>counter </td> 
     <td>Like column, but can be incremented (cannot be used with OCC) </td> 
     <td><tt>{&quot;type&quot;: &quot;counter&quot;, &quot;value&quot;: &quot;fam:qual&quot;}</tt> </td> 
    </tr> 
    <tr class="b"> 
     <td>occVersion </td> 
     <td>Use OCC (see below) </td> 
     <td><tt>{&quot;type&quot;: &quot;counter&quot;}</tt> </td> 
    </tr> 
   </tbody> 
  </table> 
  <p><i>Optimistic Concurrency</i></p> 
  <p>Optimistic Concurrency Control (OCC) allows you to do get/update/put operations, ensuring that another thread or process doesn’t update the entity between the time you fetch it and update it. OCC works by keeping a version column in each row of the table that tracks the version of the entity persisted. Versions are always increasing: every put increases the version by 1. When reading a record, the version is passed along with the record. When the record is put back to the table, if the version isn’t what it was when the entity was fetched, the put fails.</p> 
  <p><i>Example: Avro field declared as an OCC check field</i></p> 
  <div class="source"> 
   <pre>{
  &quot;name&quot;: &quot;conflictVersion&quot;,
  &quot;type&quot;: &quot;long&quot;,
  &quot;default&quot;: 0,
  &quot;mapping&quot;: { &quot;type&quot;: &quot;occVersion&quot; }
}
</pre> 
  </div> 
  <p>If an Avro entity has a mapping declared with a mapping type <tt>occVersion</tt>, operations will always use OCC on that entity.</p> 
 </div> 
 <div class="section"> 
  <h3 id="Working_with_Datasets">Working with Datasets</h3> 
  <p>These are some practical examples of using Kite tools when working with Datasets.</p> 
  <div class="section"> 
   <h4 id="Creating_Dataset_Views">Creating Dataset Views</h4> 
   <p>Views represent subsets of the <tt>Entity</tt> objects in a <tt>Dataset</tt>. A <tt>View</tt> has the same type parameter as the <tt>Dataset</tt> it is a subset of, and can produce <tt>DatasetReader</tt> and <tt>DatasetWriter</tt> instances for its subset. The readers for a View will not return any entites that are not included in it. Similarly, the writer for a View will reject any entities that are not included.</p> 
   <p>You build a View by calling methods that add a logical constraint to an existing View, starting with a <tt>Dataset</tt> as a <tt>View</tt> of all its entities. By chaining these method calls, you build views with more constraints that entities must satisfy to be included. This provides functionality similar to SQL constraints.</p> 
   <p>For example, the following code snippet constrains a dataset to users whose favorite color is orange (this is an equality constraint).</p> 
   <div class="source"> 
    <pre>View&lt;User&gt; orange = users.with(“favoriteColor”, “orange”)
</pre> 
   </div> 
   <p>The next snippet demonstrates how you define a View by taking an existing Dataset (or an existing View) and chaining another constraint to refine it. Any View can be further refined using the constraint methods. In this case, the code takes a subset of the <tt>events</tt> view from (greater than or equal to) October 4, to (less than or equal to) April 10.</p> 
   <div class="source"> 
    <pre>long OCT_4 = new org.joda.time.DateTime(2012, 10, 4, 0, 0).getMillis();
long APR_10 = new org.joda.time.DateTime(2013, 4, 10, 0, 0).getMillis();
View&lt;Event&gt; v = events.from(“timestamp”, OCT_4).to(“timestamp”, APR_10);
</pre> 
   </div> 
   <p>The <tt>fromAfter</tt> and <tt>toBefore</tt> methods are similar to <tt>from</tt> and <tt>to</tt>, but do not contain the end point values (strict less than and strict greater than).</p> 
   <p>The <a class="externalLink" href="http://kitesdk.org/docs/current/apidocs/org/kitesdk/data/RefineableView.html" title="RefinableView interface">RefinableView interface</a> defines these methods.</p> 
  </div> 
  <div class="section"> 
   <h4 id="Using_Datasets_in_MapReduce">Using Datasets in MapReduce</h4> 
   <p>You can use the <tt>DatasetKeyOutputFormat</tt> class to write to a dataset from a MapReduce job.</p> 
   <p>The <tt>DatasetKeyOutputFormat</tt> uses the key argument to pass an Entity to store in the dataset. The Value argument can only be null, because the output value type parameter is <tt>Void</tt>.</p> 
   <p><b>Note</b>: Input and output formats are experimental and do not currently support Views.</p> 
  </div> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="Entities">Entities</h2> 
 <p><i>Summary</i></p> 
 <ul> 
  <li>An entity is a record in a dataset.</li> 
  <li>Entities can be POJOs, Avro GenericRecords, or Avro generated (specific) records.</li> 
  <li>When in doubt, use GenericRecords.</li> 
 </ul> 
 <p>An <i>entity</i> is a single record. The name “entity” is used rather than “record” because the latter caries a connotation of a simple list of primitives, while the former evokes the notion of a <a class="externalLink" href="http://en.wikipedia.org/wiki/POJO" title="Plain Old Java Object">POJO</a> (e.g. in <a class="externalLink" href="http://en.wikipedia.org/wiki/Java_Persistence_API" title="Java Persistance API">JPA</a>). That said, the terms are used interchangeably. An entity can take one of three forms, per your preference:</p> 
 <ol style="list-style-type: decimal"> 
  <li> <p><b>A plain old Java object</b></p> <p>When you supply a POJO, the library uses reflection to write the object out to storage. While not the fastest, this is the easiest way to get up and running. Users are encouraged to consider Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a>s for production systems, or after they become familiar with the APIs.</p></li> 
  <li> <p><b>An <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a> GenericRecord</b></p> <p>An Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a> instance can be used to supply entities that represent a schema without using custom types for each kind of entity. These objects are easy to create and manipulate (see Avro’s <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecordBuilder.html" title="Avro - GenericRecordBuilder Class">GenericRecordBuilder class</a>), especially in code that has no knowledge of specific object types (such as libraries). Serialization of generic records is fast, but requires use of the Avro APIs. This is recommended for most users, in most cases.</p></li> 
  <li> <p><b>An Avro specific type</b></p> <p>Advanced users might choose to use Avro’s <a class="externalLink" href="http://avro.apache.org/docs/current/gettingstartedjava.html#Serializing+and+deserializing+with+code+generation" title="Avro - Serializing and deserializing with code generation">code generation</a> support to create classes that implicitly know how to serialize themselves. While the fastest of the options, this requires specialized knowledge of Avro, code generation, and handling of custom types.</p></li> 
 </ol> 
 <p>Note that entities aren’t represented by any particular type in the Data APIs. In each of the above three cases, the entities described are either simple POJOs or are Avro objects. Remember that what has been described here is only the <i>in memory</i> representation of the entity; the Data module might store the data in HDFS in a different serialization format. By default, this is the Avro data file serialization, but it can be Parquet files or an HBase format if the HBase repository is being used.</p> 
 <p>Entities can be complex types, representing data structures with a few string attributes, or as complex as necessary. See <i>Example: User entity schema and POJO class</i> for an example of a valid Avro schema, and its associated POJO.</p> 
 <p><i>Example: User entity schema and POJO class</i></p> 
 <div class="source"> 
  <pre>Avro schema (User.avsc)
-----------------------
{
  &quot;name&quot;: &quot;User&quot;,
  &quot;type&quot;: &quot;record&quot;,
  &quot;fields&quot;: [
    // two required fields.
    { &quot;name&quot;: &quot;id&quot;,          &quot;type&quot;: &quot;long&quot; },
    { &quot;name&quot;: &quot;username&quot;,    &quot;type&quot;: &quot;string&quot; },

    // emailAddress is optional; it's value can be a string or a null.
    { &quot;name&quot;: &quot;emailAddress&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;null&quot; ] },

    // friendIds is an array with elements of type long.
    { &quot;name&quot;: &quot;friendIds&quot;,   &quot;type&quot;: { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;long&quot; } },
  ]
}

User POJO (User.java)
---------------------
public class User {

  private Long id;
  private String username;
  private String emailAddress;
  private List&lt;Long&gt; friendIds;

  public User() {
    friendIds = new ArrayList&lt;Long&gt;();
  }

  public Long getId() {
    return id;
  }

  public void setId(Long id) {
    this.id = id;
  }

  public String getUsername() {
    return username;
  }

  public void setUsername(String username) {
    this.username = username;
  }

  public String getEmailAddress() {
    return emailAddress;
  }

  public void setEmailAddress(String emailAddress) {
    this.emailAddress = emailAddress;
  }

  public List&lt;Long&gt; getFriendIds() {
    return friendIds;
  }

  public void setFriendIds(List&lt;Long&gt; friendIds) {
    this.friendIds = friendIds;
  }

  /*
   * It's fine to have methods that the schema doesn't know about. They'll
   * just be ignored during serialization.
   */
  public void addFriend(Friend friend) {
    if (!friendIds.contains(friend.getId()) {
      friendIds.add(friend.getId());
    }
  }

}
</pre> 
 </div> 
 <p>Instead of defining a POJO, you can use Avro’s <tt>GenericRecordBuilder</tt> to create a generic entity that conforms to the User schema defined earlier.</p> 
 <p><i>Example: Using Avro’s GenericRecordBuilder to create a generic entity</i></p> 
 <div class="source"> 
  <pre>/*
 * Load the schema from User.avsc.
 */
Schema userSchema = new Schema.Parser().parse(new File(&quot;User.avsc&quot;));

/*
 * The GenericRecordBuilder constructs a new record and ensures that
 * all the necessary fields are set with values of an appropriate type.
 */
GenericRecord genericUser = new GenericRecordBuilder(userSchema)
  .set(&quot;id&quot;, 1L)
  .set(&quot;username&quot;, &quot;janedoe&quot;)
  .set(&quot;emailAddress&quot;, &quot;jane@doe.com&quot;)
  .set(&quot;friendIds&quot;, Collections.&lt;Long&gt;emptyList())
  .build();
</pre> 
 </div> 
</div> 
<div class="section"> 
 <h2 id="Appendix">Appendix</h2> 
 <div class="section"> 
  <h3 id="Compatibility_Statement">Compatibility Statement</h3> 
  <p>Since this is a library, you must be able to reliably determine the intended compatibility of this project. API stability and compatibility are vital to the success of this project; any deviation from the stated guarantees is a bug. This project follows the guidelines set forth by the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> and uses the same nomenclature.</p> 
  <p>Just as with CDH (and the Semantic Versioning Specification), this project makes the following compatibility guarantees:</p> 
  <ol style="list-style-type: decimal"> 
   <li>The patch version is incremented if only backward-compatible bug fixes are introduced.</li> 
   <li>The minor version is incremented when backward-compatible features are added to the public API, parts of the public API are deprecated, or when changes are made to private code. Patch level changes might also be included.</li> 
   <li>The major version is incremented when backward-incompatible changes are made. Minor and patch level changes might also be included.</li> 
   <li>Prior to version 1.0.0, no backward-compatibility is guaranteed.</li> 
  </ol> 
  <p>See the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> for more information.</p> 
  <p>Additionally, the following statements are made:</p> 
  <ul> 
   <li>The public API is defined by the Javadoc.</li> 
   <li>Some classes might be annotated with @Beta. These classes are evolving or experimental, and are not subject to the stated compatibility guarantees. They might change incompatibly in any release.</li> 
   <li>Deprecated elements of the public API are retained for two releases and then removed. Since this breaks backward compatibility, the major version is also incremented.</li> 
  </ul> 
 </div> 
</div>
			</div>
		</div>
	</div>
	</div>

	</div><!-- /container -->
	
	<!-- Footer
	================================================== -->
	<footer class="well">
		<div class="container">
			<div class="row">
				<div class="span3 bottom-nav">
					<ul class="nav nav-list">
						<li class="nav-header">Main</li>
						<li>
							<a href="../index.html" title="Home">Home </a>
						</li>
						<li>
							<a href="../release_notes.html" title="Release Notes">Release Notes </a>
						</li>
						<li>
							<a href="../apidocs/index.html" title="Javadoc">Javadoc </a>
						</li>
						<li>
							<a href="../jdiff/changes.html" title="API Diffs">API Diffs </a>
						</li>
						<li>
							<a href="../dependencies.html" title="Dependency Information">Dependency Information </a>
						</li>
						<li>
							<a href="../migrating.html" title="Migrating from CDK">Migrating from CDK </a>
						</li>
						<li>
							<a href="http://github.com/kite-sdk/kite-examples" title="Examples" class="externalLink">Examples </a>
						</li>
					</ul>
				</div>
				<div class="span3 bottom-nav">
					<ul class="nav nav-list">
						<li class="nav-header">Modules</li>
						<li>
							<a href="index.html" title="Kite Data">Kite Data </a>
						</li>
						<li>
							<a href="../kite-maven-plugin/index.html" title="Kite Maven Plugin">Kite Maven Plugin </a>
						</li>
						<li>
							<a href="../kite-morphlines/index.html" title="Kite Morphlines">Kite Morphlines </a>
						</li>
						<li>
							<a href="../kite-tools/index.html" title="Kite Tools">Kite Tools </a>
						</li>
						<li class="nav-header">Kite Data</li>
						<li>
							<a href="index.html" title="Home">Home </a>
						</li>
						<li class="active">
							<a href="#" title="Reference Guide">Reference Guide </a>
						</li>
						<li>
							<a href="design.html" title="Design">Design </a>
						</li>
						<li>
							<a href="limitations.html" title="Known Limitations">Known Limitations </a>
						</li>
						<li>
							<a href="faq.html" title="FAQ">FAQ </a>
						</li>
					</ul>
				</div>
				<div class="span3 bottom-nav">
					<ul class="nav nav-list">
						<li class="nav-header">Contribute</li>
						<li>
							<a href="http://github.com/kite-sdk/kite" title="Source Repository" class="externalLink">Source Repository </a>
						</li>
						<li>
							<a href="https://issues.cloudera.org/browse/CDK" title="JIRA" class="externalLink">JIRA </a>
						</li>
						<li>
							<a href="https://groups.google.com/a/cloudera.org/forum/#!forum/cdk-dev" title="Mailing List" class="externalLink">Mailing List </a>
						</li>
						<li>
							<a href="../how_to_contribute.html" title="How To Contribute">How To Contribute </a>
						</li>
						<li>
							<a href="../adding_a_module.html" title="Adding a Module">Adding a Module </a>
						</li>
						<li>
							<a href="../project-info.html" title="Project Information">Project Information </a>
						</li>
					</ul>
				</div>
			</div>
		</div>
	</footer>
		
	<div class="container subfooter">
		<div class="row">
			<div class="span12">
				<p class="pull-right"><a href="#">Back to top</a></p>
				<p class="copyright">Copyright &copy;2013-2014. All Rights Reserved.</p>
				<p class="version-date"><span class="projectVersion">Version: 0.12.1. </span><span class="publishDate">Last Published: 2014-03-18. </span></p>
				<p><a href="http://github.com/andriusvelykis/reflow-maven-skin" title="Reflow Maven skin">Reflow Maven skin</a> by <a href="http://andrius.velykis.lt" target="_blank" title="Andrius Velykis">Andrius Velykis</a>.</p>
			</div>
		</div>
	</div>

	<!-- Le javascript
	================================================== -->
	<!-- Placed at the end of the document so the pages load faster -->

	<!-- Fallback jQuery loading from Google CDN:
	     http://stackoverflow.com/questions/1014203/best-way-to-use-googles-hosted-jquery-but-fall-back-to-my-hosted-library-on-go -->
	<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
	<script type="text/javascript">
		if (typeof jQuery == 'undefined')
		{
			document.write(unescape("%3Cscript src='./js/jquery-1.8.3.min.js' type='text/javascript'%3E%3C/script%3E"));
		}
	</script>
	
	<script src="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>
	<script src="./js/lightbox.js"></script>
	<script src="./js/jquery.smooth-scroll.min.js"></script>
	<!-- back button support for smooth scroll -->
	<script src="./js/jquery.ba-bbq.min.js"></script>
	<script src="http://yandex.st/highlightjs/7.3/highlight.min.js"></script>

	<script src="./js/reflow-skin.js"></script>
	
	</body>
</html>
