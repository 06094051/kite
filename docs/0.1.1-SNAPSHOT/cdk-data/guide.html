<!DOCTYPE html>
<!--
 | Generated by Apache Maven Doxia at Apr 10, 2013
 | Rendered using Apache Maven Fluido Skin 1.3.0
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="Date-Revision-yyyymmdd" content="20130410" />
    <meta http-equiv="Content-Language" content="en" />
    <title>CDK Data Module - </title>
    <link rel="stylesheet" href="./css/apache-maven-fluido-1.3.0.min.css" />
    <link rel="stylesheet" href="./css/site.css" />
    <link rel="stylesheet" href="./css/print.css" media="print" />

      
    <script type="text/javascript" src="./js/apache-maven-fluido-1.3.0.min.js"></script>

    
            </head>
        <body class="topBarDisabled">
          
    
    
            
    
        
    <a href="http://github.com/cloudera/cdk">
      <img style="position: absolute; top: 0; right: 0; border: 0; z-index: 10000;"
        src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"
        alt="Fork me on GitHub">
    </a>
  
                
                    
    
        <div class="container-fluid">
          <div id="banner">
        <div class="pull-left">
                                <div id="bannerLeft">
                <h2>CDK Data Module</h2>
                </div>
                      </div>
        <div class="pull-right">  </div>
        <div class="clear"><hr/></div>
      </div>

      <div id="breadcrumbs">
        <ul class="breadcrumb">
                
            
                  <li id="publishDate">Last Published: 2013-04-10</li>
                  <li class="divider">|</li> <li id="projectVersion">Version: 0.1.1-SNAPSHOT</li>
                      
                
            
      
                            </ul>
      </div>

            
      <div class="row-fluid">
        <div id="leftColumn" class="span3">
          <div class="well sidebar-nav">
                
            
                <ul class="nav nav-list">
                    <li class="nav-header">CDK</li>
                                
      <li>
    
                          <a href="../index.html" title="Home">
          <i class="none"></i>
        Home</a>
            </li>
                  
      <li>
    
                          <a href="../faq.html" title="FAQ">
          <i class="none"></i>
        FAQ</a>
            </li>
                  
      <li>
    
                          <a href="../how_to_contribute.html" title="How To Contribute">
          <i class="none"></i>
        How To Contribute</a>
            </li>
                  
      <li>
    
                          <a href="http://github.com/cloudera/cdk" class="externalLink" title="Source Repository">
          <i class="none"></i>
        Source Repository</a>
            </li>
                  
      <li>
    
                          <a href="https://issues.cloudera.org/browse/CDK" class="externalLink" title="JIRA">
          <i class="none"></i>
        JIRA</a>
            </li>
                                        <li class="nav-header">Data Module</li>
                                
      <li class="active">
    
            <a href="#"><i class="none"></i>Reference Guide</a>
          </li>
                  
      <li>
    
                          <a href="apidocs/index.html" title="Javadoc">
          <i class="none"></i>
        Javadoc</a>
            </li>
                  
      <li>
    
                          <a href="design.html" title="Design">
          <i class="none"></i>
        Design</a>
            </li>
                  
      <li>
    
                          <a href="limitations.html" title="Known Limitations">
          <i class="none"></i>
        Known Limitations</a>
            </li>
                              <li class="nav-header">Reports</li>
                                                                                                                                                                            
      <li>
    
                          <a href="project-info.html" title="Project Information">
          <i class="icon-chevron-right"></i>
        Project Information</a>
                  </li>
                                                                    
      <li>
    
                          <a href="project-reports.html" title="Project Reports">
          <i class="icon-chevron-right"></i>
        Project Reports</a>
                  </li>
            </ul>
                
            
                
          <hr class="divider" />

           <div id="poweredBy">
                            <div class="clear"></div>
                            <div class="clear"></div>
                            <div class="clear"></div>
                             <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
        <img class="builtBy" alt="Built by Maven" src="./images/logos/maven-feather.png" />
      </a>
                  </div>
          </div>
        </div>
        
                
        <div id="bodyColumn"  class="span9" >
                                  
            <h1>CDK Data Reference Guide</h1><div class="section"><h2>About This Guide<a name="About_This_Guide"></a></h2><p>This reference guide is the primary source of documentation for the CDK Data module. It covers the high level organization of the APIs, primary classes and interfaces, intended usage, available extension points for customization, and implementation information where helpful and appropriate.</p><p>From here on, this guide assumes you are already familiar with the basic design and functionality of HDFS, Hadoop MapReduce, and Java SE 6. Users who are also familiar with <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a>, data serialization techniques, common compression algorithms (e.g. gzip, snappy), advanced Hadoop MapReduce topics (e.g. input split calculation), and traditional data management topics (e.g. partitioning schemes, metadata management) will benefit even more.</p></div><div class="section"><h2>Whats New<a name="Whatnulls_New"></a></h2><div class="section"><h3>Version 0.1.0<a name="Version_0.1.0"></a></h3><p>Version 0.1.0 is the first release of the CDK Data module. This is considered a <i>beta</i> release. As a sub-1.0.0 release, this version is <i>not</i> subject to the normal API compatibility guarantees. See the <i>Compatibility Statement</i> for information about API compatibility guarantees.</p></div></div><div class="section"><h2>Overview of the Data Module<a name="Overview_of_the_Data_Module"></a></h2><p>The CDK Data module is a set of APIs for interacting with datasets in the Hadoop ecosystem. Specifically built to simplify direct reading and writing of datasets in storage subsystems such as the Hadoop Distributed FileSystem (HDFS), the Data module provides familiar, stream-oriented APIs, that remove the complexity of data serialization, partitioning, organization, and metadata system integration. These APIs do not replace or supersede any of the existing Hadoop APIs. Instead, the Data module acts as a targetted application of those APIs for its stated use case. In other words, many applications will still use the HDFS or Avro APIs directly when the developer has use cases outside of direct dataset create, drop, read, and write operations. On the other hand, for users building applications or systems such as data integration services, the Data module will usually be superior in its default choices, data organization, and metadata system integration, when compared to custom built code.</p><p>In keeping with the overarching theme and principles of the CDK, the Data module is prescriptive. Rather than present a do-all Swiss Army knife library, this module makes specific design choices that guide users toward well-known patterns that make sense for many, if not all, cases. It is likely that advanced users with niche use cases or applications will find it difficult, suboptimal, or even impossible to do unusual things. Limiting the user is not a goal, but when revealing an option creates significant opportunity for complexity, or would otherwise require the user to delve into a rathole of additional choices or topics to research, such a tradeoff has been made. The Data module is designed to be immediately useful, obvious, and in line with what most users want, out of the box.</p><p>These APIs are designed to easily fit in with dependency injection frameworks like <a class="externalLink" href="http://www.springsource.org/spring-framework" title="Spring Framework">Spring</a> and <a class="externalLink" href="http://code.google.com/p/google-guice/" title="Google Guice">Google Guice</a>. Users can use constructor injection when using these kinds of systems. Alternatively, users who prefer not to use DI frameworks will almost certainly prefer the builder-style helper classes that come with many of the critical classes. By convention, these builders are always inner static classes named <tt>Builder</tt>, contained within their constituent classes.</p><p>The primary actors in the Data module are <i>entities</i>, <i>dataset repositories</i>, <i>datasets</i>, dataset <i>readers</i> and <i>writers</i>, and <i>metadata providers</i>. Most of these objects are interfaces, permitting multiple implementations, each with different functionality. Today, there exists an implementation of each of these components for the Hadoop FileSystem abstraction, found in the <tt>com.cloudera.data.filesystem</tt> package. While, in theory, this means any implementation of Hadoops <tt>FileSystem</tt> abstract class is supported by the Data module, only the local and HDFS filesystem implementations are tested and officially supported. For the remainder of this guide, you can assume the implementation of the Data module interfaces being described is the Hadoop <tt>FileSystem</tt> implementation.</p><p>If youre not already familiar with <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a> schemas, now is a good time to go read a little <a class="externalLink" href="http://avro.apache.org/docs/current/spec.html" title="Avro Specification">more about them</a>. You need not concern yourself with the details of how objects are serialized, but the ability to specify the schema to which entities of a dataset must conform is critical. The rest of this guide will assume you have a basic understanding of how to define a schema.</p></div><div class="section"><h2>Dataset Repositories and Metadata Providers<a name="Dataset_Repositories_and_Metadata_Providers"></a></h2><p>A <i>dataset repository</i> is a physical storage location for datasets. In keeping with the relational database analogy, a dataset repository is the equivalent of a database of tables. Developers may organize datasets into different dataset repositories for reasons related to logical grouping, security and access control, backup policies, and so forth. A dataset repository is represented by instances of the <tt>com.cloudera.data.DatasetRepository</tt> interface in the Data module. An instance of a <tt>DatasetRepository</tt> acts as a factory for datasets, supplying methods for creating, loading, and dropping datasets. Each dataset belongs to exactly one dataset repository. Theres no built in support for moving or copying datasets bewteen repositories. MapReduce and other execution engines can easily provide copy functionality, if desired.</p><p><i>DatasetRepository Interface</i></p>
<div class="source"><pre class="prettyprint">Dataset create(String, DatasetDescriptor);
Dataset get(String);
boolean drop(String);
</pre></div><p>The Data module ships with a <tt>DatasetRepository</tt> implementation <tt>com.cloudera.data.filesystem.FileSystemDatasetRepository</tt> built for operating on datasets stored in a filesystem supported by Hadoops <tt>FileSystem</tt> abstraction. This implementation requires an instance of a Hadoop <tt>FileSystem</tt>, a root directory under which datasets will be (or are) stored, and an optional <i>metadata provider</i> (described later) to be supplied upon instantiation. Once complete, users can freely interact with datasets while the implementation worries about managing files and directories in the underlying filesystem. The supplied metadata provider is used to resolve dataset schemas and other related information.</p><p>Instantiating FileSystemDatasetRepository is straight forward.</p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()),
  new Path(&quot;/data&quot;)
);
</pre></div><p>This example uses the currently configured Hadoop <tt>FileSystem</tt> implementation, typically an HDFS cluster. Since Hadoops also supports a local implementation of <tt>FileSystem</tt>, its possible to use the Data APIs to interact with data residing on a local OS filesystem. This is especially useful during development and basic functional testing of your code.</p><p>Once an instance of a <tt>DatasetRepository</tt> implementation has been created, new datasets can easily be created, and existing datasets can be loaded or dropped. Heres a more complete example of creating a dataset to store application event data. Youll notice a few new classes; dont worry about them for now. Well cover them later.</p>
<div class="source"><pre class="prettyprint">/*
 * Instantiate a FileSystemDatasetRepository with a Hadoop FileSystem object
 * based on the current configuration. The FileSystem and Configuration
 * classes come from Hadoop. The Path object tells the dataset repository
 * under what path in the configured filesystem datasets reside.
 */
DatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()),
  new Path(&quot;/data&quot;)
);

/*
 * Create the dataset &quot;users&quot; with the schema defined in the file User.avsc.
 */
Dataset users = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .get()
);
</pre></div><p>Related to the dataset repository, the <i>metadata provider</i> is a service provider interface used to interact with the service that provides dataset metadata information to the rest of the Data APIs. The <tt>com.cloudera.data.MetadataProvider</tt> interface defines the contract metadata services must provide to the library, and specifically, the <tt>DatasetRepository</tt>.</p><p><i>MetadataProvider Interface</i></p>
<div class="source"><pre class="prettyprint">void save(String, DatasetDescriptor);
DatasetDescriptor load(String);
boolean delete(String);
</pre></div><p><tt>MetadataProvider</tt> implementations act as a bridge between the Data module and centralized metadata repositories. An obvious example of this (in the Hadoop ecosystem) is <a class="externalLink" href="http://incubator.apache.org/hcatalog/" title="Apache HCatalog">HCatalog</a> and the Hive metastore. By providing an implementation that makes the necessary API calls to HCatalogs REST service, any and all datasets are immediately consumable by systems compatible with HCatalog, the storage system represented by the <tt>DatasetRepository</tt> implementation, and the format in which the data is written. As it turns out, thats a pretty tall order and, in keeping with the CDKs purpose of simplifying rather than presenting additional options, users are encouraged to 1. use HCatalog, 2. allow this library to default to snappy compressed Avro data files, and 3. use systems that also integrate with HCatalog (directly or indirectly). In this way, this library acts as a forth integration point to working with data in HDFS that is HCatalog-aware, in addition to Hive, Pig, and MapReduce input/output formats.</p><p>Users arent expected to use metadata providers directly. Instead, <tt>DatasetRepository</tt> implementations accept instances of <tt>MetadataProvider</tt> plugins, and make whatever calls are needed as users interact with the Data APIs. If youre paying close attention, youll see that we didnt specify a metadata provider when we instantiated <tt>FileSystemDatasetRepository</tt> earlier. Thats because <tt>FileSystemDatasetRepository</tt> uses an implementation of <tt>MetadataProvider</tt> called <tt>FileSystemMetadataProvider</tt> by default. Developers are free to explicitly pass a different implementation using the three argument constructor <tt>FileSystemDatasetRepository(FileSystem, Path, MetadataProvider)</tt> if they want to change this behavior.</p><p>The <tt>FileSystemMetadataProvider</tt> (also in the packge <tt>com.cloudera.data.filesystem</tt>) plugin stores dataset metadata information on a Hadoop <tt>FileSystem</tt> in a hidden directory. As with its sibling <tt>FileSystemDatasetRepository</tt>, its constructor accepts a Hadoop <tt>FileSystem</tt> object and a base directory. When metadata needs to be stored, a directory under the supplied base directory with the dataset name is created (if it doesnt yet exist), and the dataset descriptor information is serialized to a set of files in a directory named <tt>.metadata</tt>.</p><p>_Example: Explicitly configuring <tt>FileSystemDatasetRepository</tt> with <tt>FileSystemMetadataProvider</tt>_</p>
<div class="source"><pre class="prettyprint">FileSystem fileSystem = FileSystem.get(new Configuration());
Path basePath = new Path(&quot;/data&quot;);

MetadataProvider metaProvider = new FileSystemMetadataProvider(
  fileSystem, basePath);

DatasetRepository repo = new FileSystemDatasetRepository(
  fileSystem, basePath, metaProvider);
</pre></div><p>Note how the same base directory of <tt>/data</tt> is used for both the metadata provider as well as the dataset repository. This is perfectly legal. In fact, this is exactly what happens when you use the two argument form of the <tt>FileSystemDatasetRepository</tt> constructor. Configured this way, data and metadata will be stored together, side by side, on whatever filesystem Hadoop is currently configured to use. Later, when we create a dataset, well see the resultant file and directory structure created as a result of this configuration.</p></div><div class="section"><h2>Datasets<a name="Datasets"></a></h2><p><i>Summary</i></p>
<ul>
  <li>A dataset is a collection of entities.</li>
  <li>A dataset is represented by the interface <tt>Dataset</tt>.</li>
  <li>The Hadoop FileSystem implementation of a dataset
  <ul>
    <li>is stored as Snappy-compressed Avro data files by default.</li>
    <li>may support pluggable formats such as Parquet in the future.</li>
    <li>is made up of zero or more files in a directory.</li>
  </ul></li>
</ul><p>A dataset is a collection of zero or more entities (or records). All datasets have a name and an associated <i>dataset descriptor</i>. The dataset descriptor, as the name implies, describes all aspects of the dataset. Primarily, the descriptor information is the datasets required <i>schema</i> and its optional <i>partition strategy</i>. A descriptor must be provided at the time a dataset is created. The schema is defined using the Avro Schema APIs. Entities must all conform to the same schema, however, that schema can evolve based on a set of well-defined rules. The relational database analog of a dataset is a table.</p><p>Datasets are represented by the <tt>com.cloudera.data.Dataset</tt> interface. Implementations of this interface decide how to physically store the entities within the dataset. Users do not instantiate implementations of the <tt>Dataset</tt> interface directly. Instead, implementations of the <tt>DatasetRepository</tt> act as a factory of the appropriate <tt>Dataset</tt> implementation.</p><p><i>Dataset Interface</i></p>
<div class="source"><pre class="prettyprint">String getName();
DatasetDescriptor getDescriptor();

&lt;E&gt; DatasetWriter&lt;E&gt; getWriter();
&lt;E&gt; DatasetReader&lt;E&gt; getReader();

Dataset getPartition(PartitionKey, boolean);
Iterable&lt;Dataset&gt; getPartitions();
</pre></div><p>The included Hadoop <tt>FileSystemDatasetRepository</tt> includes a <tt>Dataset</tt> implementation called <tt>FileSystemDataset</tt>. This dataset implementation stores data in the configured Hadoop <tt>FileSystem</tt> as Snappy-compressed Avro data files. Avro data files were selected because all components of CDH support them, they are language agnostic, support block compression, have a compact binary representation, and are natively splittable by Hadoop MapReduce while compressed. While its not currently possible to change this behavior, this will suit the needs of almost all users, and should be used whenever possible. Support for different serialization formats is planned for future releases for cases where users know something special about their data.</p><p>Upon creation of dataset, a name and a <i>dataset descriptor</i> must be provided to the <tt>DatasetRepository#create()</tt> method. The descriptor, represented by the <tt>com.cloudera.data.DatasetDescriptor</tt> class, holds all metadata associated with the dataset, the most important of which is the schema. Schemas are always represented using Avros Schema APIs, regardless of how the data is stored by the underlying dataset implementation. This simplifies the API for users, letting them focus on a single schema definition language for all datasets. In an effort to support different styles of schema definition, the <tt>DatasetDescriptor.Builder</tt> class supports a number of convenience methods for defining or attaching a schema.</p><p><i>DatasetDescriptor Class</i></p>
<div class="source"><pre class="prettyprint">org.apache.avro.Schema getSchema();
PartitionStrategy getPartitionStrategy();
boolean isPartitioned();
</pre></div><p><i>DatasetDescriptor.Builder Class</i></p>
<div class="source"><pre class="prettyprint">Builder schema(Schema schema);
Builder schema(String json);
Builder schema(File file);
Builder schema(InputStream inputStream);
Builder schema(URL url);

Builder partitionStrategy(PartitionStrategy partitionStrategy);

DatasetDescriptor get();
</pre></div><p><i>Note</i></p><p>Some of the less important or specialized methods have been elided here in the interest of simplicity.</p><p>From the methods in the <tt>DatasetDescriptor.Builder</tt>, you can see Avro schemas can be defined in a few different ways. Here, for instance, is an example of creating a dataset with a schema defined in a file on the local filesystem.</p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = ...
Dataset users = repo.create(&quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .get()
);
</pre></div><p>Just as easily, a the schema could be loaded from a Java classpath resource. This example uses Guavas <a class="externalLink" href="http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Resources.html" title="Google Guava Resources class">Resources</a> to simplify resolution, but we could have (almost as) easily used Javas <tt>java.util.ClassLoader</tt> directly.</p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = ...
Dataset users = repo.create(&quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(Resources.getResource(&quot;Users.avsc&quot;))
    .get()
);
</pre></div><p>An instance of <tt>Dataset</tt> acts as a factory for both reader and writer streams. Each implementation is free to produce stream implementations that make sense for the underlying storage system. The <tt>FileSystemDataset</tt> implementation, for example, produces streams that read from, or write to, Avro data files on a Hadoop <tt>FileSystem</tt> implementation.</p><p>Reader and writer streams both function similarly to Javas standard IO streams, but are specialized. As indicated in the <tt>Dataset</tt> interface earlier, both interfaces are generic. The type parameter indicates the type of entity that they produce or consume, respectively.</p><p><i>DatasetReader Interface</i></p>
<div class="source"><pre class="prettyprint">void open();
void close();
boolean isOpen();

boolean hasNext();
E read();
</pre></div><p><i>DatasetWriter Interface</i></p>
<div class="source"><pre class="prettyprint">void open();
void close();
boolean isOpen();

void write(E);
void flush();
</pre></div><p>Both readers and writers are single-use objects with a well-defined lifecycle. Instances of both types (or the implementations of each, rather) must be opened prior to invoking any of the IO-generating methods such as DatasetReaders <tt>hasNext()</tt> or <tt>read()</tt>, or DatasetWriters <tt>write()</tt> or <tt>flush()</tt>. Once a stream has been closed via the <tt>close()</tt> method, no further IO is permitted, nor may it be reopened.</p><p>Writing to a dataset always follows the same sequence of events. A user obtains an instance of a <tt>Dataset</tt> from a <tt>DatasetRepository</tt> either by creating a new, or loading an existing dataset. With a reference to a <tt>Dataset</tt>, you can obtain a writer using its <tt>getWriter()</tt> method, open it, write any number of entities, flush as necessary, and close it to release resources back to the system. The use of <tt>flush()</tt> and <tt>close()</tt> can dramatically affect data durability. Implementations of the <tt>DatasetWriter</tt> interface are free to define the semantics of data durability as appropriate for their storage subsystem. See the implementation javadoc on either the streams or the dataset for more information.</p><p><i>Example: Writing to a Hadoop FileSystem</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()), new Path(&quot;/data&quot;));

/*
 * Let's assume MyInteger.avsc is defined as follows:
 * {
 *   &quot;type&quot;: &quot;record&quot;,
 *   &quot;name&quot;: &quot;MyInteger&quot;,
 *   &quot;fields&quot;: [
 *     { &quot;type&quot;: &quot;integer&quot;, &quot;name&quot;: &quot;i&quot; }
 *   ]
 * }
 */
Dataset integers = repo.create(&quot;integers&quot;,
  new DatasetDescriptor.Builder()
    .schema(&quot;MyInteger.avsc&quot;)
    .get()
);

/*
 * Getting a writer never performs IO, so it's safe to do this outside of
 * the try block. Here we're using Avro Generic records, discussed in
 * greater details later. See the Entities section.
 */
DatasetWriter&lt;GenericRecord&gt; writer = integers.getWriter();

try {
  writer.open();

  for (int i = 0; i &lt; Integer.MAX_VALUE; i++) {
    writer.write(
      new GenericRecordBuilder(integers.getDescriptor().getSchema())
        .set(&quot;i&quot;, i)
        .build()
    );
  }
} finally {
  // Always explicitly close writers!
  writer.close();
}
</pre></div><p>Reading data from an existing dataset is equally straight forward. The important part to remember is that it is illegal to call <tt>read()</tt> after the reader has been exhausted (i.e. no more entities remain). Instead, users must use the <tt>hasNext()</tt> method to test if the reader can produce further data.</p><p><i>Example: Reading from a Hadoop FileSystem</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()), new Path(&quot;/data&quot;));

// Load the integers dataset.
Dataset integers = repo.get(&quot;integers&quot;);

DatasetReader&lt;GenericRecord&gt; reader = integers.getReader();

try {
  reader.open();

  while (reader.hasNext()) {
    System.out.println(&quot;i: &quot; + reader.read().get(&quot;i&quot;));
  }
} finally {
  reader.close();
}
</pre></div><p>Dropping a dataset - an operation as equally destructive as dropping a table in a relational database - works as expected.</p><p><i>Example: Dropping an existing dataset</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()), new Path(&quot;/data&quot;));

if (repo.drop(&quot;integers&quot;)) {
  System.out.println(&quot;Dropped dataset integers&quot;);
}
</pre></div><p>As discussed earlier, all operations performed on dataset repositories, datasets, and their associated readers and writers are tightly integrated with the dataset repositorys configured metadata provider. Dropping a dataset like this, for example, removes both the data as well as the associated metadata. All applications that use the Data module APIs will automatically see changes made by one another if they share the same configuration. This is an incredibly powerful concept allowing systems to become immediately aware of data as soon as its committed to storage.</p><div class="section"><h3>Partitioned Datasets<a name="Partitioned_Datasets"></a></h3><p><i>Summary</i></p>
<ul>
  <li>Datasets may be partitioned by attributes of the entity (i.e. fields of the  record).</li>
  <li>Partitioning is transparent to readers and writers.</li>
  <li>Partitons also conform to the <tt>Dataset</tt> interface.</li>
  <li>A <tt>PartitionStrategy</tt> controls how a dataset is partitioned, and is part of  the <tt>DatasetDescriptor</tt>.</li>
</ul><p>Datasets may optionally be partitioned to facilitate piecemeal storage management, as well as optimized access to data under one or more predicates. A dataset is considered partitioned if it has an associated partition strategy (described later). When entities are written to a partitioned dataset, they are automatically written to the proper partition, as expected. The semantics of a partition are defined by the implementation; no guarantees as to the performance of reading or writing across partitions, availability of a partition in the face of failures, or the efficiency of partition elimination under one or more predicates (i.e. partition pruning in query engines) are made by the Data module interfaces. It is not possible to partition an existing non-partitioned dataset, nor can you write data into a partitioned dataset that does not land in a partition. Should you decide to partition an existing dataset, the best course of action is to create a new partitioned dataset with the same schema as the existing dataset, and use MapReduce to convert the dataset in batch to the new format. It will be possible to remove partitions from a partitioned dataset in a future release, but today, they may only be added. A partitioned dataset can provide a list of partitions (described later).</p><p>Upon creation of a dataset, a <tt>PartitionStrategy</tt> may be provided. A partition strategy is a list of one or more partition functions that, when applied to an attribute of an entity, produce a value used to decide in which partition an entity should be written. Different partition function implementations exist, each of which faciliates a different form of partitioning. The initial version of the library includes the identity and hash functions for use in partition strategies.</p><p>While users are free to instantiate the <tt>PartitionStrategy</tt> class directly, its <tt>Builder</tt> greatly simpifies life.</p><p><i>PartitionStrategy.Builder API</i></p>
<div class="source"><pre class="prettyprint">Builder identity(String, int);
Builder hash(String, int);

PartitionStrategy get();
</pre></div><p>When building a partition strategy, the attribute (i.e. field) name from which to take the function input is specified, along with a cardinality hint (or limit, in the case of the hash function). For example, given the Avro schema for a <tt>User</tt> entity with a <tt>segment</tt> attribute of type <tt>long</tt>, a partition strategy that uses the identity function on the <tt>segment</tt> attribute will effectively bucket users by their segment value.</p><p><i>Sample User Avro Schema (User.avsc)</i></p>
<div class="source"><pre class="prettyprint">{
  &quot;name&quot;: &quot;User&quot;,
  &quot;type&quot;: &quot;record&quot;,
  &quot;fields&quot;: [
    { &quot;name&quot;: &quot;id&quot;,           &quot;type&quot;: &quot;long&quot;   },
    { &quot;name&quot;: &quot;username&quot;,     &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot;: &quot;emailAddress&quot;, &quot;type&quot;: &quot;string&quot; },
    { &quot;name&quot;: &quot;segment&quot;,      &quot;type&quot;: &quot;long&quot;   }
  ]
}
</pre></div><p><i>Example Creation of a dataset partitioned by an attribute</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = ...

Dataset usersDataset = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .partitionStrategy(
      new PartitionStrategy.Builder().identity(&quot;segment&quot;, 1024).get()
    ).get()
);
</pre></div><p>Given the ficticious User entities shown in <i>Example: Sample Users</i>, users A, B, and C would be written to partition 1, while D and E end up in partition 2.</p><p><i>Example: Sample Users</i></p>
<div class="source"><pre class="prettyprint">id  username  emailAddress  segment
--  --------  ------------  -------
100 A         A@a.com       1
101 B         B@b.com       1
102 C         C@c.com       1
103 D         D@d.com       2
104 E         E@e.com       2
</pre></div><p>Partitioning is not limited to a single attribute of an entity.</p><p><i>Example: Creation of a dataset partitioned by multiple attributes</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository repo = ...

Dataset users = repo.create(
  &quot;users&quot;,
  new DatasetDescriptor.Builder()
    .schema(new File(&quot;User.avsc&quot;))
    .partitionStrategy(
      new PartitionStrategy.Builder()
        .identity(&quot;segment&quot;, 1024)  // Partition first by segment
        .hash(&quot;emailAddress&quot;, 3)    // and then by hash(email) % 3
        .get()
    ).get()
</pre></div><p>The order in which the partition functions are defined is important. This controls the way the data is physically partitioned in certain implementations of the Data APIs. Depending on the implementation, this can drastically change the execution speed of data access by different methods.</p><p><b>Warning</b></p><p>Its worth pointing out that Hive and Impala only support the identity function in partitioned datasets, at least at the time this is written. Users who do not use partitioning for subset selection may use any partition function(s) they choose. If, however, you wish to use the partition pruning in Hive/Impalas query engine, only the identity function will work. This is because both systems rely on the idea that the value in the path name equals the value found in each record. To mimic more complex partitioning schemes, users often resort to adding a surrogate field to each record to hold the dervived value and handle proper setting of such a field themselves.</p></div></div><div class="section"><h2>Entities<a name="Entities"></a></h2><p><i>Summary</i></p>
<ul>
  <li>An entity is a record in a dataset.</li>
  <li>Entities can be POJOs, Avro GenericRecords, or Avro generated (specific)  records.</li>
  <li>When in doubt, use GenericRecords.</li>
</ul><p>An <i>entity</i> is a is a single record. The name entity is used rather than record because the latter caries a connotation of a simple list of primitives, while the former evokes the notion of a <a class="externalLink" href="http://en.wikipedia.org/wiki/POJO" title="Plain Old Java Object">POJO</a> (e.g. in <a class="externalLink" href="http://en.wikipedia.org/wiki/Java_Persistence_API" title="Java Persistance API">JPA</a>). That said, the terms are used interchangably. An entity can take one of three forms, at the users option:</p>
<ol style="list-style-type: decimal">
  <li>A plain old Java object</li>
</ol><p>When a POJO is supplied, the library uses reflection to write the object out  to storage. While not the fastest, this is the simplest way to get up and  running. Users are encouraged to consider Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a>s for  production systems, or after they become familiar with the APIs.</p>
<ol style="list-style-type: decimal">
  <li>An <a class="externalLink" href="http://avro.apache.org" title="Apache Avro">Avro</a> GenericRecord</li>
</ol><p>An Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a> instance can be used to easily supply  entities that represent a schema without using custom types for each kind of  entity. These objects are easy to create and manipulate (see Avros  <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecordBuilder.html" title="Avro - GenericRecordBuilder Class">GenericRecordBuilder class</a>), especially in code that has no  knowledge of specific object types (such as libraries). Serialization of  generic records is fast, but requires use of the Avro APIs. This is  recommended for most users, in most cases.</p>
<ol style="list-style-type: decimal">
  <li>An Avro specific type</li>
</ol><p>Advanced users may choose to use Avros <a class="externalLink" href="http://avro.apache.org/docs/current/gettingstartedjava.html#Serializing+and+deserializing+with+code+generation" title="Avro - Serializing and deserializing with code generation">code generation</a> support to  create classes that implicitly know how to serialize themselves. While the  fastest of the options, this requires specialized knowledge of Avro, code  generation, and handling of custom types. Keep in mind that, unlike generic  records, the applications that write datasets with specific types must also  have the same classes available to the applications that read those datasets.</p><p>Note that entities arent represented by any particular type in the Data APIs. In each of the above three cases, the entities described are either simple POJOs or are Avro objects. Remember that what has been described here is only the _in memory_ representation of the entity; the Data module may store the data in HDFS in a different serialization format.</p><p>Entites may be complex types, representing data structures as simple as a few string attributes, or as complex as necessary. See _Example: User entity schema and POJO class_ for an example of a valid Avro schema, and its associated POJO.</p><p><i>Example: User entity schema and POJO class</i></p>
<div class="source"><pre class="prettyprint">Avro schema (User.avsc)
-----------------------
{
  &quot;name&quot;: &quot;User&quot;,
  &quot;type&quot;: &quot;record&quot;,
  &quot;fields&quot;: [
    // two required fields.
    { &quot;name&quot;: &quot;id&quot;,          &quot;type&quot;: &quot;long&quot; },
    { &quot;name&quot;: &quot;username&quot;,    &quot;type&quot;: &quot;string&quot; },

    // emailAddress is optional; it's value can be a string or a null.
    { &quot;name&quot;: &quot;emailAddress&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;null&quot; ] },

    // friendIds is an array with elements of type long.
    { &quot;name&quot;: &quot;friendIds&quot;,   &quot;type&quot;: { &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;long&quot; } },
  ]
}

User POJO (User.java)
---------------------
public class User {

  private Long id;
  private String username;
  private String emailAddress;
  private List&lt;Long&gt; friendIds;

  public User() {
    friendIds = new ArrayList&lt;Long&gt;();
  }

  public Long getId() {
    return id;
  }

  public void setId(Long id) {
    this.id = id;
  }

  public String getUsername() {
    return username;
  }

  public void setUsername(String username) {
    this.username = username;
  }

  public String getEmailAddress() {
    return emailAddress;
  }

  public void setEmailAddress(String emailAddress) {
    this.emailAddress = emailAddress;
  }

  public List&lt;Long&gt; getFriendIds() {
    return friendIds;
  }

  public void setFriendIds(List&lt;Long&gt; friendIds) {
    this.friendIds = friendIds;
  }

  /*
   * It's fine to have methods that the schema doesn't know about. They'll
   * just be ignored during serialization.
   */
  public void addFriend(Friend friend) {
    if (!friendIds.contains(friend.getId()) {
      friendIds.add(friend.getId());
    }
  }

}
</pre></div><p>Instead of defining a POJO, we could also use Avros <tt>GenericRecordBuilder</tt> to create a generic entity that conforms to the User schema we defined earlier.</p><p><i>Example: Using Avros GenericRecordBuilder to create a generic entity</i></p>
<div class="source"><pre class="prettyprint">/*
 * Load the schema from User.avsc. Later, we'll an easier way to reference
 * Avro schemas when working with the CDK Data APIs.
 */
Schema userSchema = new Schema.Parser().parse(new File(&quot;User.avsc&quot;));

/*
 * The GenericRecordBuilder constructs a new record and ensures that we set
 * all the necessary fields with values of an appropriate type.
 */
GenericRecord genericUser = new GenericRecordBuilder(userSchema)
  .set(&quot;id&quot;, 1L)
  .set(&quot;username&quot;, &quot;janedoe&quot;)
  .set(&quot;emailAddress&quot;, &quot;jane@doe.com&quot;)
  .set(&quot;friendIds&quot;, Collections.&lt;Long&gt;emptyList())
  .build();
</pre></div><p>Later, well see how to read and write these entities to a dataset.</p></div><div class="section"><h2>Appendix<a name="Appendix"></a></h2><div class="section"><h3>Compatibility Statement<a name="Compatibility_Statement"></a></h3><p>As a library, users must be able to reliably determine the intended compatibility of this project. We take API stability and compatibility seriously; any deviation from the stated guarantees is a bug. This project follows the guidelines set forth by the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> and uses the same nomenclature.</p><p>Just as with CDH (and the Semantic Versioning Specification), this project makes the following compatibility guarantees:</p>
<ol style="list-style-type: decimal">
  <li>The patch version is incremented if only backward-compatible bug fixes are  introduced.</li>
  <li>The minor version is incremented when backward-compatible features are added  to the public API, parts of the public API are deprecated, or when changes  are made to private code. Patch level changes may also be included.</li>
  <li>The major version is incremented when backward-incompatible changes are made.  Minor and patch level changes may also be included.</li>
  <li>Prior to version 1.0.0, no backward-compatibility is guaranteed.</li>
</ol><p>See the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> for more information.</p><p>Additionally, the following statements are made:</p>
<ul>
  <li>The public API is defined by the Javadoc.</li>
  <li>Some classes may be annotated with @Beta. These classes are evolving or  experimental, and are not subject to the stated compatibility guarantees. They  may change incompatibly in any release.</li>
  <li>Deprecated elements of the public API are retained for two releases and then  removed. Since this breaks backward compatibility, the major version must also  be incremented.</li>
</ul></div></div>
                  </div>
            </div>
          </div>

    <hr/>

    <footer>
            <div class="container-fluid">
              <div class="row span12">Copyright &copy;                    2013
                        <a href="http://www.cloudera.com">Cloudera</a>.
            All Rights Reserved.      
            
      </div>

        
        
                </div>
    </footer>
  </body>
</html>
