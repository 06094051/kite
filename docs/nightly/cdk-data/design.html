<!DOCTYPE html>
<!--
 | Generated by Apache Maven Doxia at Apr 14, 2013
 | Rendered using Apache Maven Fluido Skin 1.3.0
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="Date-Revision-yyyymmdd" content="20130414" />
    <meta http-equiv="Content-Language" content="en" />
    <title>CDK Data Module - </title>
    <link rel="stylesheet" href="./css/apache-maven-fluido-1.3.0.min.css" />
    <link rel="stylesheet" href="./css/site.css" />
    <link rel="stylesheet" href="./css/print.css" media="print" />

      
    <script type="text/javascript" src="./js/apache-maven-fluido-1.3.0.min.js"></script>

    
            </head>
        <body class="topBarDisabled">
          
    
    
            
    
        
    <a href="http://github.com/cloudera/cdk">
      <img style="position: absolute; top: 0; right: 0; border: 0; z-index: 10000;"
        src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"
        alt="Fork me on GitHub">
    </a>
  
                
                    
    
        <div class="container-fluid">
          <div id="banner">
        <div class="pull-left">
                                <div id="bannerLeft">
                <h2>CDK Data Module</h2>
                </div>
                      </div>
        <div class="pull-right">  </div>
        <div class="clear"><hr/></div>
      </div>

      <div id="breadcrumbs">
        <ul class="breadcrumb">
                
            
                  <li id="publishDate">Last Published: 2013-04-14</li>
                  <li class="divider">|</li> <li id="projectVersion">Version: 0.1.1-SNAPSHOT</li>
                      
                
            
      
                            </ul>
      </div>

            
      <div class="row-fluid">
        <div id="leftColumn" class="span3">
          <div class="well sidebar-nav">
                
            
                <ul class="nav nav-list">
                    <li class="nav-header">CDK</li>
                                
      <li>
    
                          <a href="../index.html" title="Home">
          <i class="none"></i>
        Home</a>
            </li>
                  
      <li>
    
                          <a href="http://github.com/cloudera/cdk" class="externalLink" title="Source Repository">
          <i class="none"></i>
        Source Repository</a>
            </li>
                  
      <li>
    
                          <a href="https://issues.cloudera.org/browse/CDK" class="externalLink" title="JIRA">
          <i class="none"></i>
        JIRA</a>
            </li>
                  
      <li>
    
                          <a href="../how_to_contribute.html" title="How To Contribute">
          <i class="none"></i>
        How To Contribute</a>
            </li>
                  
      <li>
    
                          <a href="../adding_a_module.html" title="Adding a Module">
          <i class="none"></i>
        Adding a Module</a>
            </li>
                                        <li class="nav-header">Data Module</li>
                                
      <li>
    
                          <a href="guide.html" title="Reference Guide">
          <i class="none"></i>
        Reference Guide</a>
            </li>
                  
      <li>
    
                          <a href="apidocs/index.html" title="Javadoc">
          <i class="none"></i>
        Javadoc</a>
            </li>
                  
      <li class="active">
    
            <a href="#"><i class="none"></i>Design</a>
          </li>
                  
      <li>
    
                          <a href="limitations.html" title="Known Limitations">
          <i class="none"></i>
        Known Limitations</a>
            </li>
                  
      <li>
    
                          <a href="faq.html" title="FAQ">
          <i class="none"></i>
        FAQ</a>
            </li>
                              <li class="nav-header">Reports</li>
                                                                                                                                                                            
      <li>
    
                          <a href="project-info.html" title="Project Information">
          <i class="icon-chevron-right"></i>
        Project Information</a>
                  </li>
                                                                    
      <li>
    
                          <a href="project-reports.html" title="Project Reports">
          <i class="icon-chevron-right"></i>
        Project Reports</a>
                  </li>
            </ul>
                
            
                
          <hr class="divider" />

           <div id="poweredBy">
                            <div class="clear"></div>
                            <div class="clear"></div>
                            <div class="clear"></div>
                             <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
        <img class="builtBy" alt="Built by Maven" src="./images/logos/maven-feather.png" />
      </a>
                  </div>
          </div>
        </div>
        
                
        <div id="bodyColumn"  class="span9" >
                                  
            <h1>CDK Data Module</h1><p>This is the design draft for the CDK Data module.</p><p><b>WARNING WARNING WARNING</b></p><p>This is the original design document for the CDK data module. It is <i>not</i> kept up to date with the current state of the code base. It's here to provide context and insight into why the decisions that were made were made, but it's not for most users. Instead, check out the <a href="guide.html">reference guide</a>.</p><p><b>WARNING WARNING WARNING</b></p><div class="section"><h2>Overview<a name="Overview"></a></h2><p>Today, interacting with data in Hadoop means working with files and directories. For users who are used to thinking in terms of tables and databases, this is surprising low-level and complicated. Seemingly simple tasks such as organizing and managing data hierarchies present a barrier to adoption. Selecting from the matrix of file formats, compression codecs, tool support, and serialization formats stumps just about every developer new to the platform. If we could reduce this decision set by providing experience in library form, we could reduce another barrier to adoption, and increase usability.</p><p>The CDK Data module is a Java library intended to solve this exact set of problems. By providing a logical abstraction consisting of <i>dataset repositories</i>, <i>datasets</i>, <i>entities</i>, and reader/writer streams, users can get on with programmatic direct dataset access and management. The intention and expectation is that this library will be useful, primarily, to those writing data integration tools, either custom, or ISV-provided, but also anyone that needs to touch data in HDFS directly (i.e. not by way of MapReduce or higher level engines such as Impala). Accessibility and integration with the rest of the platform are the two primary objectives. In other words, this library is one level below Hadoop's input/output formats for MapReduce, but above the standard HDFS+<a class="externalLink" href="http://avro.apache.org/" title="Apache Avro">Avro</a> APIs.</p><p>The Data APIs are primarily a set of generic interfaces and classes for storing and retrieving entities (i.e. records) to and from datasets (i.e. tables). Datasets are, in turn, grouped into dataset repositories (i.e. databases). Reads and writes are performed using streams that are always attached to a dataset. The inital implementation will be focused on large, sequential operations.</p><p>Implementations of the generic interfaces can exist for different storage systems. Today, the only implementation is built on top of Hadoop's FileSystem abstraction which means anything FileSystem supports, the Data APIs should support. The local filesystem and HDFS implementations are the primary testing targets.</p><p>There is some logical overlap with existing systems that's worth pointing out, however this library is, in fact, complementary to all of these systems. This is discussed in the context of each system, in detail, in the section <i>Related Work and Systems</i>.</p><div class="section"><h3>Entities<a name="Entities"></a></h3><p>Summary:</p>
<ul>
  <li>An entity is a record in a dataset.</li>
  <li>Entities can be POJOs, GenericRecords, or generated (specific) records.</li>
  <li>When in doubt, use GenericRecords.</li>
</ul><p>An <i>entity</i> is a is a single record. The name entity is used rather than record because the latter caries a connotation of a simple list of primitives, while the former evokes the notion of a <a class="externalLink" href="http://en.wikipedia.org/wiki/POJO" title="Plain Old Java Object">POJO</a> (e.g. in <a class="externalLink" href="http://en.wikipedia.org/wiki/Java_Persistence_API" title="Java Persistance API">JPA</a>). An entity can take one of three forms, at the user's option:</p>
<ol style="list-style-type: decimal">
  <li>A plain old Java object</li>
</ol><p>When a POJO is supplied, the library uses reflection to write the object out  to storage. While not the fastest, this is the simplest way to get up and  running. Users are encourage to consider Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a>s for  production systems, or after they become familiar with the APIs.</p>
<ol style="list-style-type: decimal">
  <li>An <a class="externalLink" href="http://avro.apache.org/" title="Apache Avro">Avro</a> GenericRecord</li>
</ol><p>An Avro <a class="externalLink" href="http://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericRecord.html" title="Avro - GenericRecord Interface">GenericRecord</a> instance can be used to easily supply  entities that represent a schema. These objects are easy to create and  manipulate, especially in code that has no knowledge of specific object types  (such as libraries). Serialization of generic records is fast, but requires  use of the Avro APIs. This is recommended for most users.</p>
<ol style="list-style-type: decimal">
  <li>An Avro specific type</li>
</ol><p>Advanced users may choose to use Avro's <a class="externalLink" href="http://avro.apache.org/docs/current/gettingstartedjava.html#Serializing+and+deserializing+with+code+generation" title="Avro - Serializing and deserializing with code generation">code generation</a> support to  create classes that implicitly know how to serialize themselves. While the  fastest of the options, this requires specialized knowledge of Avro, code  generation, and handling of custom types.</p></div><div class="section"><h3>Datasets<a name="Datasets"></a></h3><p>Summary:</p>
<ul>
  <li>A dataset is a collection of entities.</li>
  <li>A dataset may be partitioned by attributes of the entity (i.e. fields of the  record).</li>
  <li>A dataset is represented by the interface <tt>Dataset</tt>.</li>
  <li>The Hadoop FileSystem implementation of a dataset
  <ul>
    <li>is stored as Snappy-compressed Avro by default.</li>
    <li>may support pluggable formats such as Parquet in the future.</li>
    <li>is made up of zero or more files in a directory.</li>
  </ul></li>
</ul><p>A dataset is a collection of zero or more entities. All datasets have a name and an associated <i>dataset descriptor</i>. The dataset descriptor, as the name implies, describes all aspects of the dataset. Primarily, the descriptor information is the dataset's required <i>schema</i> and its optional <i>partition strategy</i>. A descriptor must be provided at the time a dataset is created. The schema is defined using the Avro Schema APIs. Entities must all conform to the same schema, however, that schema can evolve based on a set of well-defined rules. The relational analog of a dataset is a table.</p><p>While Avro is used to define the schema, it is possible that the underlying storage format of the data is not Avro data files. This is because other constraints may apply, based on the subsystems or access patterns. The implementing class is expected to translate the Avro schema into whatever is appropriate for the underlying system. Implementations, of course, are free to use Avro serialization if it makes sense. The current thinking is that Avro data files will be the default for data in HDFS.</p><p>Datasets may optionally be partitioned to facilitate piecemeal storage management, as well as optimized access to data under one or more predicates. A dataset is considered partitioned if it has an associated partition strategy (described later). When records are written to a partitioned dataset, they are automatically written to the proper partition, as expected. The semantics of a partition are defined by the implementation; this interface makes no guarantee as to the performance of reading or writing across partitions, availability of a partition in the face of failures, or the efficiency of partition elimination under one or more predicates (i.e. partition pruning in query engines). It is not possible to partition an existing non-partitioned dataset, nor can one write data into a partitioned dataset that does not land in a partition. It is possible to add or remove partitions from a partitioned dataset. A partitioned dataset can provide a list of partitions (described later).</p><p><i>The DatasetDescriptor API</i></p>
<div class="source"><pre class="prettyprint">getSchema(): org.apache.avro.Schema
getPartitionStrategy(): PartitionStrategy
isPartitioned(): boolean
</pre></div><p>Datasets are never instantiated by users, directly. Instead, they are created using factory methods on a <tt>DatasetRepository</tt> (described later).</p><p>An instance of <tt>Dataset</tt> acts as a factory for both reader and writer streams. Each implementation is free to produce stream implementations that make sense for the underlying storage system. The Hadoop <tt>FileSystem</tt> implementation, for example, produces streams that read from, or write to, Avro data files on a <tt>FileSystem</tt> implementation.</p><p><i><tt>Dataset</tt> stream factory methods</i></p>
<div class="source"><pre class="prettyprint">&lt;E&gt; getReader(): DatasetReader&lt;E&gt;
&lt;E&gt; getWriter(): DatasetWriter&lt;E&gt;
</pre></div><p>Reader and writer streams both function similarly to Java's standard IO streams, but are specialized. As indicated above, both interfaces are generic. The type parameter indicates the type of entity that they produce or consume, respectively.</p><p><i>Stream interfaces</i></p>
<div class="source"><pre class="prettyprint">DatasetReader&lt;E&gt;

    open()
    close()
    isOpen(): boolean

    hasNext(): boolean
    read(): E

DatasetWriter&lt;E&gt;

    open()
    close()
    isOpen(): boolean

    write(E)
    flush()
</pre></div><p>Upon creation of a dataset, a <tt>PartitionStrategy</tt> may be provided. A partition strategy is a list of one or more partition functions that, when applied to an attribute of an entity, produce a value used to decide in which partition an entity should be written. Different partition function implementations exist, each of which faciliates a different form of partitioning. The initial version of the library includes the identity, hash, range, and value list functions.</p><p><i>A Partitioned Dataset Example</i></p>
<div class="source"><pre class="prettyprint">/* Assume the content of userSchema is defined as follows:
 * {
 *   &quot;type&quot;: &quot;record&quot;,
 *   &quot;name&quot;: &quot;User&quot;,
 *   &quot;fields&quot;: [
 *     { &quot;type&quot;: &quot;long&quot;, &quot;name&quot;: &quot;userId&quot; },
 *     { &quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;username&quot; }
 *   ]
 * }
 */
Schema userSchema = ...;

FileSystemDatasetRepository repo = new FileSystemDatasetRepository(
  FileSystem.get(new Configuration()),
  new Path(&quot;/data&quot;),
  new FileSystemMetadataProvider(fileSystem, new Path(&quot;/data&quot;))
);

DatasetDescriptor desc = new DatasetDescriptor.Builder()
  .schema(userSchema)
  .partitionStrategy(
    /*
     * Partition the users dataset using the hash code of the value of the
     * userId attribute modulo 53.
     */
    new PartitionStrategy.Builder().hash(&quot;userId&quot;, 53).get()
  ).get();

Dataset users = repo.create(&quot;users&quot;, desc);
DatasetWriter[Record] writer = users.getWriter();

try {
  writer.open();

  /*
   * This writes to /data/users/data/userId=15/*.avro because
   * (Integer.valueOf(1234).hashCode() &amp; Integer.MAX_VALUE) % 53 = 15
   */
  writer.write(
    new GenericRecordBuilder(userSchema)
      .set(&quot;userId&quot;, 1234)
      .set(&quot;username&quot;, &quot;jane&quot;)
      .build()
  );

  writer.flush();
} finally {
  writer.close();
}
</pre></div><p>This example produces a dataset that, when written to, may have up to 53 partitions. User entities written to the dataset will be automatically written to the correct partition. Note that the name of the attribute used in the partition strategy builder (userId) must appear in the schema (user.avsc). Multiple partition functions may be specified. The order of specification is extremely important as it reflects the physical storage (in the case of the Hadoop FileSystem implementation).</p><p>It's worth pointing out that Hive and Impala only support the identity function in partitioned datasets, at least at the time this is written. Users who do not use partitioning for subset selection may use any partition function(s) they choose. If, however, you wish to use the partition pruning in Hive/Impala's query engine, only the identity function will work. This is because both systems rely on the idea that the value in the path name equals the value found in each record. If you look closely at the earlier example, you'll see that while the value of the userId attribute in record is 1234, its value in the path is 15. To mimic more complex partitioning schemes, users often resort to adding a surrogate field to each record to hold the dervived value and handle proper setting of such a field themselves.</p><p>The equivalent workaround for the hashed field example above is to add a new attribute to the User entity called <tt>userIdHash</tt>, set it to the proper value in user code, and use the identity function on that column instead. Note that this means partition pruning is no longer transparent; the user must know to query the table using <tt>... WHERE userIdHash = (hashCode(SOME_VALUE) % 53)</tt>. The hope is that these engines learn about more complex partitioning schemes in the future.</p></div><div class="section"><h3>Dataset Repositories and Metadata Providers<a name="Dataset_Repositories_and_Metadata_Providers"></a></h3><p>A <i>dataset repository</i> is a physical storage location for zero or more datasets. In keeping with the relational database analogy, a dataset repository is the equivalent of a database. An instance of a DatasetRepository acts as a factory for Datasets, supplying methods for creating, loading, and dropping datasets. Each dataset belongs to exactly one dataset repository. There's no built in support for moving or copying datasets bewteen repositories. MapReduce and other execution engines can easily provide copy functionality if it's desirable.</p><p><i>DatasetRepository APIs</i></p>
<div class="source"><pre class="prettyprint">DatasetRepository

    get(String): Dataset
    create(String, DatasetDescriptor): Dataset
    drop(String): boolean
</pre></div><p>Along with the Hadoop FileSystem <tt>Dataset</tt> and stream implementations, a related <tt>DatasetRepository</tt> implementation exists. This implementation requires an instance of a Hadoop <tt>FileSystem</tt>, a root directory under which datasets will be (or are) stored, and a metadata provider to be supplied upon instantiation. Once complete, users can freely interact with datasets under the supplied root directory. The supplied <tt>MetadataProvider</tt> is used to resolve dataset schemas, partitioning information, and any other like data.</p><p>Along with the dataset repository, the <i>metadata provider</i> is a service provider interface used to interact with the service that provides dataset metadata information. The MetadataProvider interface defines the contract metadata services must provide to the library, and specifically, the <tt>DatasetRepository</tt>.</p><p><i>MetadataProvider API</i></p>
<div class="source"><pre class="prettyprint">MetadataProvider

    save(String, DatasetDescriptor)
    load(String): DatasetDescriptor
    delete(String): boolean
</pre></div><p>The expectation is that MetadataProvider implementations will act as a bridge between this library and centralized metadata repositories. An obvious example of this (in the Hadoop ecosystem) is <a class="externalLink" href="http://incubator.apache.org/hcatalog/" title="Apache HCatalog">HCatalog</a> and the Hive metastore. By providing an implementation that makes the necessary API calls to HCatalog's REST service, any and all datasets are immediately consumable by systems compatible with HCatalog, the storage system represented by the DatasetRepository implementation, and the format in which the data is written. As it turns out, that's a pretty tall order and, in keeping with the CDK's purpose of simplifying rather than presenting additional options, users are encouraged to 1. use HCatalog, 2. allow this library to default to snappy compressed Avro data files, and 3. use systems that also integrate with HCatalog. In this way, this library acts as a forth integration point to working with data in HDFS that is HCatalog-aware, in addition to Hive, Pig, and MapReduce input/output formats.</p><p>At this time, an HCatalog implementation of the <tt>MetadataProvider</tt> interface does not exist. It is, however, straight forward to implement and on the roadmap.</p></div></div><div class="section"><h2>Logistics<a name="Logistics"></a></h2><div class="section"><h3>Project Format<a name="Project_Format"></a></h3><p>The intention is for this library to be released as an open source project, under the Apache Software License. The project will be hosted and managed on Github. Community contributions are welcome and are encouraged. Those that wish to contribute to the project <i>must</i> complete a contributor license agreement (CLA) and provide their changes to the project under the same license as the rest of the project. This must be done prior to accepting any changes (e.g. a Github pull request).</p><p>A more detailed How to Contribute document, as well as individual and corporate CLAs, will be provided when with the initial publication of the project.</p><p>All feature requests and bugs are tracked in the CDK JIRA project at <a class="externalLink" href="https://issues.cloudera.org/browse/CDK">https://issues.cloudera.org/browse/CDK</a>.</p><p>Users are encouraged to use the <a class="externalLink" href="https://groups.google.com/a/cloudera.org/forum/?fromgroups#!forum/cdh-user">cdh-user@cloudera.org mailing list</a> to discuss CDK-related topics.</p></div><div class="section"><h3>Releases<a name="Releases"></a></h3><p>Since this project ultimately produces a Java library, the natural way to disseminate releases by way of <a class="externalLink" href="http://repository.cloudera.com">Cloudera's Maven repository</a>. Direct downloads containing the combined source and binary artifacts will also be provided. Optionally, we may additionally publish artifacts to Maven Central.</p><p>Release frequency is left undefined at this time. That said, since this project makes similar compatibility gurantees as CDH (see <i>Compatibility Statement</i> ), quarterly releases seem likely.</p></div><div class="section"><h3>Compatibility Statement<a name="Compatibility_Statement"></a></h3><p>As a library, users must be able to reliably determine the intended compatibility of this project. We take API stability and compatibility seriously; any deviation from the stated guarantees is a bug. This project follows the guidelines set forth by the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> and uses the same nomenclature.</p><p>Just as with CDH (and semver), this project makes the following compatibility guarantees:</p>
<ol style="list-style-type: decimal">
  <li>The patch version is incremented if only backward-compatible bug fixes are  introduced.</li>
  <li>The minor version is incremented when backward-compaatible features are added  to the public API, parts of the public API are deprecated, or when changes  are made to private code. Patch level changes may also be included.</li>
  <li>The major version is incremented when backward-incompatible changes are made.  Minor and patch level changes may also be included.</li>
  <li>Prior to version 1.0.0, no backward-compatibility is guaranteed.</li>
</ol><p>See the <a class="externalLink" href="http://semver.org/">Semantic Versioning Specification</a> for more information.</p><p>Additionally, the following statements are made:</p>
<ul>
  <li>The public API is defined by the Javadoc.</li>
  <li>Some classes may be annotated with @Beta. These classes are evolving or  experimental, and are not subject to the stated compatibility guarantees. They  may change incompatibly in any release.</li>
  <li>Deprecated elements of the public API are retained for two releases and then  removed. Since this breaks backward compatibility, the major version must also  be incremented.</li>
</ul></div></div><div class="section"><h2>Examples<a name="Examples"></a></h2><p>The goal here is to demonstrate the possible APIs, not serve as a user reference. In other words, the actual APIs may differ; no attempt will be made to keep these examples up to date with the actual implementation.</p><p><b>Writing to a new dataset</b></p>
<div class="source"><pre class="prettyprint">FileSystem fileSystem = FileSystem.get(new Configuration());
Schema eventSchema = new Schema.Parser.parse(
  Resources.getResource(&quot;event.avsc&quot;).openStream()
);

DatasetRepository repo = new FileSystemDatasetRepository(
  fileSystem,
  new Path(&quot;/data&quot;),
  new FileSystemMetadataProvider(fileSystem, new Path(&quot;/data&quot;))
);
DatasetDescriptor eventDescriptor = new DatasetDescriptor.Builder()
  .schema(eventSchema)
  .get();
Dataset events = repo.create(&quot;events&quot;, eventDescriptor);
DatasetWriter&lt;Event&gt; writer = events.getWriter();

try {
  writer.open();

  while (...) {
    /*
     * Event is an Avro specific (generated) type, a generic type, or a
     * POJO, in which case we use reflection. Here, we use a POJO.
     */
    Event e = ...

    writer.write(e);
  }
} finally {
  if (writer != null) {
    writer.close();
  }
}
</pre></div><p><b>Reading from existing an existing dataset</b></p>
<div class="source"><pre class="prettyprint">FileSystem fileSystem = FileSystem.get(new Configuration());

DatasetRepository repo = new FileSystemDatasetRepository(
  fileSystem,
  new Path(&quot;/data&quot;)
  new FileSystemMetadataProvider(fileSystem, new Path(&quot;/data&quot;))
);
Dataset events = repo.get(&quot;events&quot;);
DatasetReader&lt;GenericData.Record&gt; reader = events.getReader();

try {
  reader.open();

  while (reader.hasNext()) {
    // We can also use Avro Generic records.
    GenericData.Record record = reader.read();

    System.out.println(new StringBuilder(&quot;event - timestamp:&quot;)
      .append(record.get(&quot;timestamp&quot;))
      .append(&quot; eventId:&quot;, record.get(&quot;eventId&quot;))
      .append(&quot; message:&quot;, record.get(&quot;message&quot;))
      .toString()
    );
  }
} finally {
  reader.close();
}
</pre></div></div><div class="section"><h2>Related Work and Systems<a name="Related_Work_and_Systems"></a></h2><p><b>HDFS APIs</b></p><p>The HDFS APIs are used by the Data module. This seems safe as the <tt>FileSystem</tt> API is relatively stable. This API, however, is much higher level than the typical HDFS streams so users aren't worried about bytes.</p><p><b>Avro, Protocol Buffers, Thrift</b></p><p>The Data APIs standardize on Avro's in-memory representation of a schema, but make no promise about the underlying storage format. That said, Avro satisfies the set of criteria for optimally storing data in HDFS, and the platform team has already done a bunch of work to make it work with all components. In this way, the Data APIs are a layer above the file format.</p><p><b>Kiji</b></p><p>WibiData's Kiji schema is an HBase-only library that overlaps with the Data module's schema tracking, but is much more prescriptive about how a user interacts with its readers and writers all the way up the stack. That is, Kiji entities are not simple Avro entities that are already supported by platform components. Special input / output formats are required in order to be able to use the Kiji-ified records. Further, Kiji only supports HBase for data, and assumes HBase is available for storage of schemas. Since we have different requirements and a different use case, we see this as a separate concern.</p><p><b>HCatalog</b></p><p>See the <i>Dataset Repositories and Metadata Providers</i> section for information about integration plans and compatibility with HCatalog.</p></div>
                  </div>
            </div>
          </div>

    <hr/>

    <footer>
            <div class="container-fluid">
              <div class="row span12">Copyright &copy;                    2013
                        <a href="http://www.cloudera.com">Cloudera</a>.
            All Rights Reserved.      
            
      </div>

        
        
                </div>
    </footer>
  </body>
</html>
